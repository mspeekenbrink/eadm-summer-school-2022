
@article{beesley_uncertainty_2015,
  title = {Uncertainty and Predictiveness Determine Attention to Cues during Human Associative Learning},
  author = {Beesley, Tom and Nguyen, Katherine P. and Pearson, Daniel and Le Pelley, Mike E.},
  year = {2015},
  month = nov,
  journal = {Quarterly Journal of Experimental Psychology},
  volume = {68},
  number = {11},
  pages = {2175--2199},
  publisher = {{SAGE Publications}},
  issn = {1747-0218},
  doi = {10.1080/17470218.2015.1009919},
  abstract = {Prior research has suggested that attention is determined by exploiting what is known about the most valid predictors of outcomes and exploring those stimuli that are associated with the greatest degree of uncertainty about subsequent events. Previous studies of human contingency learning have revealed evidence for one or other of these processes, but differences in the designs and procedures of these studies make it difficult to pinpoint the crucial determinant of whether attentional exploitation or exploration will dominate. Here we present two studies in which we systematically manipulated both the predictiveness of cues and uncertainty regarding the outcomes with which they were associated. This allowed us to demonstrate, for the first time, evidence of both attentional exploration and exploitation within the same experiment. Moreover, while the effect of predictiveness persisted to influence the rate of novel learning about the same cues in a second stage, the effect of uncertainty did not. This suggests that attentional exploration is more sensitive to a change of context than is exploitation. The pattern of data is simulated with a hybrid attentional model.},
  langid = {english},
  keywords = {Associabilty,Associative learning,Attention,Eye tracking,Uncertainty},
  file = {/home/maarten/Zotero/storage/DYDM8EHH/Beesley et al. - 2015 - Uncertainty and predictiveness determine attention.pdf}
}

@article{behrens_learning_2007,
  title = {Learning the Value of Information in an Uncertain World},
  author = {Behrens, Timothy E. J. and Woolrich, Mark W. and Walton, Mark E. and Rushworth, Matthew F. S.},
  year = {2007},
  month = sep,
  journal = {Nature Neuroscience},
  volume = {10},
  number = {9},
  pages = {1214--1221},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/nn1954},
  abstract = {Our decisions are guided by outcomes that are associated with decisions made in the past. However, the amount of influence each past outcome has on our next decision remains unclear. To ensure optimal decision-making, the weight given to decision outcomes should reflect their salience in predicting future outcomes, and this salience should be modulated by the volatility of the reward environment. We show that human subjects assess volatility in an optimal manner and adjust decision-making accordingly. This optimal estimate of volatility is reflected in the fMRI signal in the anterior cingulate cortex (ACC) when each trial outcome is observed. When a new piece of information is witnessed, activity levels reflect its salience for predicting future outcomes. Furthermore, variations in this ACC signal across the population predict variations in subject learning rates. Our results provide a formal account of how we weigh our different experiences in guiding our future actions.},
  copyright = {2007 Nature Publishing Group},
  langid = {english},
  keywords = {Animal Genetics and Genomics,Behavioral Sciences,Biological Techniques,Biomedicine,general,Neurobiology,Neurosciences},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research},
  file = {/home/maarten/Zotero/storage/5QXFQKYM/Behrens et al. - 2007 - Learning the value of information in an uncertain .pdf;/home/maarten/Zotero/storage/UNMG5C6K/nn1954.html}
}

@article{bland_different_2012,
  title = {Different {{Varieties}} of {{Uncertainty}} in {{Human Decision-Making}}},
  author = {Bland, Amy and {schaefer}, alexandre},
  year = {2012},
  journal = {Frontiers in Neuroscience},
  volume = {6},
  pages = {85},
  issn = {1662-453X},
  doi = {10.3389/fnins.2012.00085},
  abstract = {The study of uncertainty in decision-making is receiving greater attention in the fields of cognitive and computational neuroscience. Several lines of evidence are beginning to elucidate different variants of uncertainty. Particularly, risk, ambiguity, and expected and unexpected forms of uncertainty are well articulated in the literature. In this article we review both empirical and theoretical evidence arguing for the potential distinction between three forms of uncertainty; expected uncertainty, unexpected uncertainty, and volatility. Particular attention will be devoted to exploring the distinction between unexpected uncertainty and volatility which has been less appreciated in the literature. This includes evidence mainly from neuroimaging, neuromodulation, and electrophysiological studies. We further address the possible differentiation of cognitive control mechanisms used to deal with these forms of uncertainty. Finally, we explore whether the dual modes of control theory provides a theoretical framework for understanding the distinction between unexpected uncertainty and volatility.},
  file = {/home/maarten/Zotero/storage/2UB5QPW2/Bland and schaefer - 2012 - Different Varieties of Uncertainty in Human Decisi.pdf}
}

@article{BROWN200949,
  title = {Detecting and Predicting Changes},
  author = {Brown, Scott D. and Steyvers, Mark},
  year = {2009},
  journal = {Cognitive Psychology},
  volume = {58},
  number = {1},
  pages = {49--67},
  issn = {0010-0285},
  doi = {10.1016/j.cogpsych.2008.09.002},
  abstract = {When required to predict sequential events, such as random coin tosses or basketball free throws, people reliably use inappropriate strategies, such as inferring temporal structure when none is present. We investigate the ability of observers to predict sequential events in dynamically changing environments, where there is an opportunity to detect true temporal structure. In two experiments we demonstrate that participants often make correct statistical decisions when asked to infer the hidden state of the data generating process. However, when asked to make predictions about future outcomes, accuracy decreased even though normatively correct responses in the two tasks were identical. A particle filter model accounts for all data, describing performance in terms of a plausible psychological process. By varying the number of particles, and the prior belief about the probability of a change occurring in the data generating process, we were able to model most of the observed individual differences.},
  keywords = {Change detection,Gamblerâ€™s Fallacy,Hot hand,Particle filters,Random sequences},
  file = {/home/maarten/Zotero/storage/5SFDEP3C/Detecting and predicting changes  Elsevier Enhanc.pdf}
}

@article{camerer_recent_1992,
  title = {Recent Developments in Modeling Preferences: {{Uncertainty}} and Ambiguity},
  shorttitle = {Recent Developments in Modeling Preferences},
  author = {Camerer, Colin and Weber, Martin},
  year = {1992},
  month = oct,
  journal = {Journal of Risk and Uncertainty},
  volume = {5},
  number = {4},
  pages = {325--370},
  issn = {1573-0476},
  doi = {10.1007/BF00122575},
  abstract = {In subjective expected utility (SEU), the decision weights people attach to events are their beliefs about the likelihood of events. Much empirical evidence, inspired by Ellsberg (1961) and others, shows that people prefer to bet on events they know more about, even when their beliefs are held constant. (They are averse to ambiguity, or uncertainty about probability.) We review evidence, recent theoretical explanations, and applications of research on ambiguity and SEU.},
  langid = {english},
  file = {/home/maarten/Zotero/storage/7MNDCGRE/Camerer and Weber - 1992 - Recent developments in modeling preferences Uncer.pdf}
}

@article{cogliati_dezza_learning_2017,
  title = {Learning the Value of Information and Reward over Time When Solving Exploration-Exploitation Problems},
  author = {Cogliati Dezza, Irene and Yu, Angela J. and Cleeremans, Axel and Alexander, William},
  year = {2017},
  month = dec,
  journal = {Scientific Reports},
  volume = {7},
  number = {1},
  pages = {16919},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-017-17237-w},
  abstract = {To flexibly adapt to the demands of their environment, animals are constantly exposed to the conflict resulting from having to choose between predictably rewarding familiar options (exploitation) and risky novel options, the value of which essentially consists of obtaining new information about the space of possible rewards (exploration). Despite extensive research, the mechanisms that subtend the manner in which animals solve this exploitation-exploration dilemma are still poorly understood. Here, we investigate human decision-making in a gambling task in which the informational value of each trial and the reward potential were separately manipulated. To better characterize the mechanisms that underlined the observed behavioural choices, we introduce a computational model that augments the standard reward-based reinforcement learning formulation by associating a value to information. We find that both reward and information gained during learning influence the balance between exploitation and exploration, and that this influence was dependent on the reward context. Our results shed light on the mechanisms that underpin decision-making under uncertainty, and suggest new approaches for investigating the exploration-exploitation dilemma throughout the animal kingdom.},
  copyright = {2017 The Author(s)},
  langid = {english},
  keywords = {Decision,Learning algorithms},
  annotation = {Bandiera\_abtest: a Cc\_license\_type: cc\_by Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Decision;Learning algorithms Subject\_term\_id: decision;learning-algorithms},
  file = {/home/maarten/Zotero/storage/SGVP8KZD/Cogliati Dezza et al. - 2017 - Learning the value of information and reward over .pdf;/home/maarten/Zotero/storage/464PTAF9/s41598-017-17237-w.html}
}

@article{cohen_should_2007,
  title = {Should {{I}} Stay or Should {{I}} Go? {{How}} the Human Brain Manages the Trade-off between Exploitation and Exploration},
  shorttitle = {Should {{I}} Stay or Should {{I}} Go?},
  author = {Cohen, Jonathan D and McClure, Samuel M and Yu, Angela J},
  year = {2007},
  month = may,
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {362},
  number = {1481},
  pages = {933--942},
  publisher = {{Royal Society}},
  doi = {10.1098/rstb.2007.2098},
  abstract = {Many large and small decisions we make in our daily lives\textemdash which ice cream to choose, what research projects to pursue, which partner to marry\textemdash require an exploration of alternatives before committing to and exploiting the benefits of a particular choice. Furthermore, many decisions require re-evaluation, and further exploration of alternatives, in the face of changing needs or circumstances. That is, often our decisions depend on a higher level choice: whether to exploit well known but possibly suboptimal alternatives or to explore risky but potentially more profitable ones. How adaptive agents choose between exploitation and exploration remains an important and open question that has received relatively limited attention in the behavioural and brain sciences. The choice could depend on a number of factors, including the familiarity of the environment, how quickly the environment is likely to change and the relative value of exploiting known sources of reward versus the cost of reducing uncertainty through exploration. There is no known generally optimal solution to the exploration versus exploitation problem, and a solution to the general case may indeed not be possible. However, there have been formal analyses of the optimal policy under constrained circumstances. There have also been specific suggestions of how humans and animals may respond to this problem under particular experimental conditions as well as proposals about the brain mechanisms involved. Here, we provide a brief review of this work, discuss how exploration and exploitation may be mediated in the brain and highlight some promising future directions for research.},
  keywords = {decision making,exploration,learning,neurotransmitters,prefrontal cortex,uncertainty},
  file = {/home/maarten/Zotero/storage/LXHDCEJL/Cohen et al. - 2007 - Should I stay or should I go How the human brain .pdf}
}

@article{courville_bayesian_2006,
  title = {Bayesian Theories of Conditioning in a Changing World},
  author = {Courville, Aaron C. and Daw, Nathaniel D. and Touretzky, David S.},
  year = {2006},
  month = jul,
  journal = {Trends in Cognitive Sciences},
  series = {Special Issue: {{Probabilistic}} Models of Cognition},
  volume = {10},
  number = {7},
  pages = {294--300},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2006.05.004},
  abstract = {The recent flowering of Bayesian approaches invites the re-examination of classic issues in behavior, even in areas as venerable as Pavlovian conditioning. A statistical account can offer a new, principled interpretation of behavior, and previous experiments and theories can inform many unexplored aspects of the Bayesian enterprise. Here we consider one such issue: the finding that surprising events provoke animals to learn faster. We suggest that, in a statistical account of conditioning, surprise signals change and therefore uncertainty and the need for new learning. We discuss inference in a world that changes and show how experimental results involving surprise can be interpreted from this perspective, and also how, thus understood, these phenomena help constrain statistical theories of animal and human learning.},
  langid = {english},
  file = {/home/maarten/Zotero/storage/DL4K8D4Y/Courville et al. - 2006 - Bayesian theories of conditioning in a changing wo.pdf;/home/maarten/Zotero/storage/S6IB4CWB/S1364661306001288.html}
}

@article{daw_cortical_2006,
  title = {Cortical Substrates for Exploratory Decisions in Humans},
  author = {Daw, Nathaniel D. and O'Doherty, John P. and Dayan, Peter and Seymour, Ben and Dolan, Raymond J.},
  year = {2006},
  month = jun,
  journal = {Nature},
  volume = {441},
  number = {7095},
  pages = {876--879},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature04766},
  abstract = {Humans are remarkably curious, and that is useful in helping us to learn about new environments and possibilities. But curiosity killed the cat, they say, and it also carries with it substantial potential risks and costs for us. Statisticians, engineers and economists have long considered ways of balancing the costs and benefits of exploration. Tests involving a gambling task and an fMRI brain scanner now show that humans appear to obey similar principles when considering their options. The players had to balance the desire to select the richest option based on accumulated experience against the desire to choose a less familiar option that might have a larger payoff. The frontopolar cortex, a brain area known to be involved in cognitive control, was preferentially active during exploratory decisions. The results suggest a neurobiological account of human exploration and point to a new area for behavioural and neural investigations.},
  copyright = {2006 Nature Publishing Group},
  langid = {english},
  keywords = {Humanities and Social Sciences,multidisciplinary,Science},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research},
  file = {/home/maarten/Zotero/storage/TP8QBN56/Daw et al. - 2006 - Cortical substrates for exploratory decisions in h.pdf;/home/maarten/Zotero/storage/FRB7WBD2/nature04766.html}
}

@article{dayan_learning_2000,
  title = {Learning and Selective Attention},
  author = {Dayan, Peter and Kakade, Sham and Montague, P. Read},
  year = {2000},
  month = nov,
  journal = {Nature Neuroscience},
  volume = {3},
  number = {11},
  pages = {1218--1223},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/81504},
  abstract = {Selective attention involves the differential processing of different stimuli, and has widespread psychological and neural consequences. Although computational modeling should offer a powerful way of linking observable phenomena at different levels, most work has focused on the relatively narrow issue of constraints on processing resources. By contrast, we consider statistical and informational aspects of selective attention, divorced from resource constraints, which are evident in animal conditioning experiments involving uncertain predictions and unreliable stimuli. Neuromodulatory systems and limbic structures are known to underlie attentional effects in such tasks.},
  copyright = {2000 Nature America Inc.},
  langid = {english},
  keywords = {Animal Genetics and Genomics,Behavioral Sciences,Biological Techniques,Biomedicine,general,Neurobiology,Neurosciences},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Reviews},
  file = {/home/maarten/Zotero/storage/D4MS47NB/Dayan et al. - 2000 - Learning and selective attention.pdf;/home/maarten/Zotero/storage/N46KSRGM/nn1100_1218.html}
}

@article{dayan_uncertainty_2003,
  title = {Uncertainty and {{Learning}}},
  author = {Dayan, Peter and Yu, Angela},
  year = {2003},
  month = mar,
  journal = {IETE Journal of Research},
  volume = {49},
  number = {2-3},
  pages = {171--181},
  publisher = {{Taylor \& Francis}},
  issn = {0377-2063},
  doi = {10.1080/03772063.2003.11416335},
  abstract = {It is a commonplace in statistics that uncertainty about parameters drives learning. Indeed one of the most influential models of behavioural learning has uncertainty at its heart. However, many popular theoretical models of learning focus exclusively on error, and ignore uncertainty. Here we review the links between learning and uncertainty from three perspectives: statistical theories such as the Kalman filter, psychological models in which differential attention is paid to stimuli with an effect on the speed of learning associated with those stimuli, and neurobiological data on the influence of the neuromodulators acetylcholine and norepinephrine on learning and inference.},
  keywords = {Acetylcholine,Attention,Kalman filter,Learning,Neuromodulation,Norepinephrine,Uncertainty},
  annotation = {\_eprint: https://doi.org/10.1080/03772063.2003.11416335},
  file = {/home/maarten/Zotero/storage/QWZEIRNQ/Dayan and Jyu - 2003 - Uncertainty and Learning.pdf;/home/maarten/Zotero/storage/4C5864Q7/03772063.2003.html}
}

@article{de_martino_confidence_2013,
  title = {Confidence in Value-Based Choice},
  author = {De Martino, Benedetto and Fleming, Stephen M. and Garrett, Neil and Dolan, Raymond J.},
  year = {2013},
  month = jan,
  journal = {Nature Neuroscience},
  volume = {16},
  number = {1},
  pages = {105--110},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/nn.3279},
  abstract = {This study examines the neural coding of decision confidence when human subjects make value-based economic choices, and finds that signals of explicit confidence are encoded in the activity of ventromedial prefrontal cortex and its interaction with the rostrolateral prefrontal cortex.},
  copyright = {2012 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  keywords = {Computational neuroscience,Decision},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Computational neuroscience;Decision Subject\_term\_id: computational-neuroscience;decision},
  file = {/home/maarten/Zotero/storage/2LX9LSQV/De Martino et al. - 2013 - Confidence in value-based choice.pdf;/home/maarten/Zotero/storage/Z4SBI6BK/nn.html}
}

@article{edwards_practical_2019,
  title = {Practical {{Calculation}} of {{Gittins Indices}} for {{Multi-armed Bandits}}},
  author = {Edwards, James},
  year = {2019},
  month = sep,
  journal = {arXiv:1909.05075 [cs, stat]},
  eprint = {1909.05075},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Gittins indices provide an optimal solution to the classical multi-armed bandit problem. An obstacle to their use has been the common perception that their computation is very difficult. This paper demonstrates an accessible general methodology for the calculating Gittins indices for the multi-armed bandit with a detailed study on the cases of Bernoulli and Gaussian rewards. With accompanying easy-to-use open source software, this work removes computation as a barrier to using Gittins indices in these commonly found settings.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/maarten/Zotero/storage/YG29SIFW/Edwards - 2019 - Practical Calculation of Gittins Indices for Multi.pdf;/home/maarten/Zotero/storage/JHYVYLFA/1909.html}
}

@article{ellsberg_risk_1961,
  title = {Risk, {{Ambiguity}}, and the {{Savage Axioms}}},
  author = {Ellsberg, Daniel},
  year = {1961},
  journal = {The Quarterly Journal of Economics},
  volume = {75},
  number = {4},
  pages = {643--669},
  publisher = {{Oxford University Press}},
  issn = {0033-5533},
  doi = {10.2307/1884324},
  abstract = {I. Are there uncertainties that are not risks? 643.--II. Uncertainties that are not risks, 647.--III. Why are some uncertainties not risks?--656.},
  file = {/home/maarten/Zotero/storage/6QX75A5L/Ellsberg - 1961 - Risk, Ambiguity, and the Savage Axioms.pdf}
}

@article{frank_prefrontal_2009,
  title = {Prefrontal and Striatal Dopaminergic Genes Predict Individual Differences in Exploration and Exploitation},
  author = {Frank, Michael J. and Doll, Bradley B. and {Oas-Terpstra}, Jen and Moreno, Francisco},
  year = {2009},
  month = aug,
  journal = {Nature Neuroscience},
  volume = {12},
  number = {8},
  pages = {1062--1068},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/nn.2342},
  abstract = {The exploration/exploitation dilemma describes the choice between maintaining the current strategy, or trying new strategies, to maximize rewards. The authors show that genes controlling striatal dopamine function are associated with exploitative learning. In contrast, a gene controlling prefrontal dopamine function is predictive of exploration when the value of alternative strategies is uncertain.},
  copyright = {2009 Nature Publishing Group},
  langid = {english},
  keywords = {Animal Genetics and Genomics,Behavioral Sciences,Biological Techniques,Biomedicine,general,Neurobiology,Neurosciences},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research},
  file = {/home/maarten/Zotero/storage/FF3LRTP7/Frank et al. - 2009 - Prefrontal and striatal dopaminergic genes predict.pdf;/home/maarten/Zotero/storage/TR3PQ9TT/nn.html}
}

@article{gallistel_extinction_2012,
  title = {Extinction from a Rationalist Perspective},
  author = {Gallistel, C. R.},
  year = {2012},
  month = may,
  journal = {Behavioural Processes},
  series = {Society for the {{Quantitative Analyses}} of {{Behavior}}: {{Extinction}}},
  volume = {90},
  number = {1},
  pages = {66--80},
  issn = {0376-6357},
  doi = {10.1016/j.beproc.2012.02.008},
  abstract = {The merging of the computational theory of mind and evolutionary thinking leads to a kind of rationalism, in which enduring truths about the world have become implicit in the computations that enable the brain to cope with the experienced world. The dead reckoning computation, for example, is implemented within the brains of animals as one of the mechanisms that enables them to learn where they are (Gallistel, 1990, Gallistel, 1995). It integrates a velocity signal with respect to a time signal. Thus, the manner in which position and velocity relate to one another in the world is reflected in the manner in which signals representing those variables are processed in the brain. I use principles of information theory and Bayesian inference to derive from other simple principles explanations for: (1) the failure of partial reinforcement to increase reinforcements to acquisition; (2) the partial reinforcement extinction effect; (3) spontaneous recovery; (4) renewal; (5) reinstatement; (6) resurgence (aka facilitated reacquisition). Like the principle underlying dead-reckoning, these principles are grounded in analytic considerations. They are the kind of enduring truths about the world that are likely to have shaped the brain's computations.},
  langid = {english},
  keywords = {Acquisition,Bayesian inference,Extinction,Information theory,Partial reinforcement,Reinstatement,Renewal,Resurgence,Spontaneous recovery},
  file = {/home/maarten/Zotero/storage/CDXJYHTJ/Gallistel - 2012 - Extinction from a rationalist perspective.pdf;/home/maarten/Zotero/storage/CX4NDAC8/S0376635712000447.html}
}

@article{gershman_deconstructing_2018,
  title = {Deconstructing the Human Algorithms for Exploration},
  author = {Gershman, Samuel J.},
  year = {2018},
  month = apr,
  journal = {Cognition},
  volume = {173},
  pages = {34--42},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2017.12.014},
  abstract = {The dilemma between information gathering (exploration) and reward seeking (exploitation) is a fundamental problem for reinforcement learning agents. How humans resolve this dilemma is still an open question, because experiments have provided equivocal evidence about the underlying algorithms used by humans. We show that two families of algorithms can be distinguished in terms of how uncertainty affects exploration. Algorithms based on uncertainty bonuses predict a change in response bias as a function of uncertainty, whereas algorithms based on sampling predict a change in response slope. Two experiments provide evidence for both bias and slope changes, and computational modeling confirms that a hybrid model is the best quantitative account of the data.},
  langid = {english},
  keywords = {Bayesian inference,Explore-exploit dilemma,Reinforcement learning},
  file = {/home/maarten/Zotero/storage/K2B9ZZ95/Gershman - 2018 - Deconstructing the human algorithms for exploratio.pdf;/home/maarten/Zotero/storage/DJTBWBDJ/S0010027717303359.html}
}

@article{gershman_uncertainty_2019,
  title = {Uncertainty and Exploration},
  author = {Gershman, Samuel J.},
  year = {2019},
  journal = {Decision},
  volume = {6},
  number = {3},
  pages = {277--286},
  publisher = {{Educational Publishing Foundation}},
  address = {{US}},
  issn = {2325-9973},
  doi = {10.1037/dec0000101},
  abstract = {In order to discover the most rewarding actions, agents must collect information about their environment, potentially foregoing reward. The optimal solution to this ``explore\textendash exploit'' dilemma is often computationally challenging, but principled algorithmic approximations exist. These approximations utilize uncertainty about action values in different ways. Some random exploration algorithms scale the level of choice stochasticity with the level of uncertainty. Other directed exploration algorithms add a ``bonus'' to action values with high uncertainty. Random exploration algorithms are sensitive to total uncertainty across actions, whereas directed exploration algorithms are sensitive to relative uncertainty. This article reports a multiarmed bandit experiment in which total and relative uncertainty were orthogonally manipulated. We found that humans employ both exploration strategies, and that these strategies are independently controlled by different uncertainty computations. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  keywords = {Algorithms,Environment,Inference,Learning,Reinforcement,Rewards,Statistical Probability,Uncertainty},
  file = {/home/maarten/Zotero/storage/N8S8U2RP/Gershman - 2019 - Uncertainty and exploration.pdf;/home/maarten/Zotero/storage/2BV95IAG/2018-48589-001.html}
}

@article{ghavamzadeh_bayesian_2015,
  title = {Bayesian {{Reinforcement Learning}}: {{A Survey}}},
  shorttitle = {Bayesian {{Reinforcement Learning}}},
  author = {Ghavamzadeh, Mohammad and Mannor, Shie and Pineau, Joelle and Tamar, Aviv},
  year = {2015},
  journal = {Foundations and Trends\textregistered{} in Machine Learning},
  volume = {8},
  number = {5-6},
  eprint = {1609.04436},
  eprinttype = {arxiv},
  pages = {359--483},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000049},
  abstract = {Bayesian methods for machine learning have been widely investigated, yielding principled methods for incorporating prior information into inference algorithms. In this survey, we provide an in-depth review of the role of Bayesian methods for the reinforcement learning (RL) paradigm. The major incentives for incorporating Bayesian reasoning in RL are: 1) it provides an elegant approach to action-selection (exploration/exploitation) as a function of the uncertainty in learning; and 2) it provides a machinery to incorporate prior knowledge into the algorithms. We first discuss models and methods for Bayesian inference in the simple single-step Bandit model. We then review the extensive recent literature on Bayesian methods for model-based RL, where prior information can be expressed on the parameters of the Markov model. We also present Bayesian methods for model-free RL, where priors are expressed over the value function or policy class. The objective of the paper is to provide a comprehensive survey on Bayesian RL algorithms and their theoretical and empirical properties.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/maarten/Zotero/storage/B9NDN45H/Ghavamzadeh et al. - 2015 - Bayesian Reinforcement Learning A Survey.pdf}
}

@article{gottlieb_attention_2014,
  title = {Attention, {{Reward}}, and {{Information Seeking}}},
  author = {Gottlieb, Jacqueline and Hayhoe, Mary and Hikosaka, Okihide and Rangel, Antonio},
  year = {2014},
  month = nov,
  journal = {Journal of Neuroscience},
  volume = {34},
  number = {46},
  pages = {15497--15504},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3270-14.2014},
  abstract = {Decision making is thought to be guided by the values of alternative options and involve the accumulation of evidence to an internal bound. However, in natural behavior, evidence accumulation is an active process whereby subjects decide when and which sensory stimulus to sample. These sampling decisions are naturally served by attention and rapid eye movements (saccades), but little is known about how saccades are controlled to guide future actions. Here we review evidence that was discussed at a recent symposium, which suggests that information selection involves basal ganglia and cortical mechanisms and that, across different contexts, it is guided by two central factors: the gains in reward and gains in information (uncertainty reduction) associated with sensory cues.},
  chapter = {Symposium},
  copyright = {Copyright \textcopyright{} 2014 the authors 0270-6474/14/3415497-08\$15.00/0},
  langid = {english},
  pmid = {25392517},
  file = {/home/maarten/Zotero/storage/WI39A6NJ/Gottlieb et al. - 2014 - Attention, Reward, and Information Seeking.pdf;/home/maarten/Zotero/storage/XTFQB9QQ/15497.html}
}

@article{hogarth_attention_2008,
  title = {Attention and Expectation in Human Predictive Learning: {{The}} Role of Uncertainty},
  shorttitle = {Attention and Expectation in Human Predictive Learning},
  author = {Hogarth, Lee and Dickinson, Anthony and Austin, Alison and Brown, Craig and Duka, Theodora},
  year = {2008},
  month = nov,
  journal = {The Quarterly Journal of Experimental Psychology},
  volume = {61},
  number = {11},
  pages = {1658--1668},
  publisher = {{Routledge}},
  issn = {1747-0218},
  doi = {10.1080/17470210701643439},
  abstract = {Three localized, visual pattern stimuli were trained as predictive signals of auditory outcomes. One signal partially predicted an aversive noise in Experiment 1 and a neutral tone in Experiment 2, whereas the other signals consistently predicted either the occurrence or absence of the noise. The expectation of the noise was measured during each signal presentation, and only participants for whom this expectation demonstrated contingency knowledge showed differential attention to the signals. Importantly, when attention was measured by visual fixations, the contingency-aware group attended more to the partially predictive signal than to the consistent predictors in both experiments. This profile of visual attention supports the Pearce and Hall (1980) theory of the role of attention in associative learning.},
  keywords = {Attention,Expectancy,Human predictive learning,Prediction,Uncertainty},
  annotation = {\_eprint: https://doi.org/10.1080/17470210701643439},
  file = {/home/maarten/Zotero/storage/2CEWTZ4Y/Hogarth et al. - 2008 - Attention and expectation in human predictive lear.pdf}
}

@article{hsu_neural_2005,
  title = {Neural {{Systems Responding}} to {{Degrees}} of {{Uncertainty}} in {{Human Decision-Making}}},
  author = {Hsu, Ming and Bhatt, Meghana and Adolphs, Ralph and Tranel, Daniel and Camerer, Colin F.},
  year = {2005},
  month = dec,
  journal = {Science},
  volume = {310},
  number = {5754},
  pages = {1680--1683},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.1115327},
  file = {/home/maarten/Zotero/storage/2NAQE2Y9/Hsu et al. - 2005 - Neural Systems Responding to Degrees of Uncertaint.pdf}
}

@article{knox_nature_2012,
  title = {The {{Nature}} of {{Belief-Directed Exploratory Choice}} in {{Human Decision-Making}}},
  author = {Knox, W. and Otto, A. and Stone, Peter and Love, Bradley},
  year = {2012},
  journal = {Frontiers in Psychology},
  volume = {2},
  pages = {398},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2011.00398},
  abstract = {In non-stationary environments, there is a conflict between exploiting currently favored options and gaining information by exploring lesser-known options that in the past have proven less rewarding. Optimal decision-making in such tasks requires considering future states of the environment (i.e., planning) and properly updating beliefs about the state of the environment after observing outcomes associated with choices. Optimal belief-updating is reflective in that beliefs can change without directly observing environmental change. For example, after 10\,s elapse, one might correctly believe that a traffic light last observed to be red is now more likely to be green. To understand human decision-making when rewards associated with choice options change over time, we develop a variant of the classic ``bandit'' task that is both rich enough to encompass relevant phenomena and sufficiently tractable to allow for ideal actor analysis of sequential choice behavior. We evaluate whether people update beliefs about the state of environment in a reflexive (i.e., only in response to observed changes in reward structure) or reflective manner. In contrast to purely ``random'' accounts of exploratory behavior, model-based analyses of the subjects' choices and latencies indicate that people are reflective belief updaters. However, unlike the Ideal Actor model, our analyses indicate that people's choice behavior does not reflect consideration of future environmental states. Thus, although people update beliefs in a reflective manner consistent with the Ideal Actor, they do not engage in optimal long-term planning, but instead myopically choose the option on every trial that is believed to have the highest immediate payoff.},
  file = {/home/maarten/Zotero/storage/7VQN4ZL7/Knox et al. - 2012 - The Nature of Belief-Directed Exploratory Choice i.pdf}
}

@article{korn_maintaining_2015,
  title = {Maintaining {{Homeostasis}} by {{Decision-Making}}},
  author = {Korn, Christoph W. and Bach, Dominik R.},
  year = {2015},
  month = may,
  journal = {PLOS Computational Biology},
  volume = {11},
  number = {5},
  pages = {e1004301},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004301},
  abstract = {Living organisms need to maintain energetic homeostasis. For many species, this implies taking actions with delayed consequences. For example, humans may have to decide between foraging for high-calorie but hard-to-get, and low-calorie but easy-to-get food, under threat of starvation. Homeostatic principles prescribe decisions that maximize the probability of sustaining appropriate energy levels across the entire foraging trajectory. Here, predictions from biological principles contrast with predictions from economic decision-making models based on maximizing the utility of the endpoint outcome of a choice. To empirically arbitrate between the predictions of biological and economic models for individual human decision-making, we devised a virtual foraging task in which players chose repeatedly between two foraging environments, lost energy by the passage of time, and gained energy probabilistically according to the statistics of the environment they chose. Reaching zero energy was framed as starvation. We used the mathematics of random walks to derive endpoint outcome distributions of the choices. This also furnished equivalent lotteries, presented in a purely economic, casino-like frame, in which starvation corresponded to winning nothing. Bayesian model comparison showed that\textemdash in both the foraging and the casino frames\textemdash participants' choices depended jointly on the probability of starvation and the expected endpoint value of the outcome, but could not be explained by economic models based on combinations of statistical moments or on rank-dependent utility. This implies that under precisely defined constraints biological principles are better suited to explain human decision-making than economic models based on endpoint utility maximization.},
  langid = {english},
  keywords = {Decision making,Economic models,Foraging,Gambling,Homeostasis,Random walk,Reaction time,Skewness},
  file = {/home/maarten/Zotero/storage/M4Y5SBX4/Korn and Bach - 2015 - Maintaining Homeostasis by Decision-Making.pdf;/home/maarten/Zotero/storage/824UQH42/article.html}
}

@article{krajbich_visual_2010,
  title = {Visual Fixations and the Computation and Comparison of Value in Simple Choice},
  author = {Krajbich, Ian and Armel, Carrie and Rangel, Antonio},
  year = {2010},
  month = oct,
  journal = {Nature Neuroscience},
  volume = {13},
  number = {10},
  pages = {1292--1298},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1726},
  doi = {10.1038/nn.2635},
  abstract = {To choose between two options, we often look repeatedly back and forth between them, presumably as a way of comparing their values. Here the authors propose a computational model of value-based decision making that can explain the relationship between fixation patterns and choices.},
  copyright = {2010 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  keywords = {Computational neuroscience,Decision,Visual system},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Computational neuroscience;Decision;Visual system Subject\_term\_id: computational-neuroscience;decision;visual-system},
  file = {/home/maarten/Zotero/storage/VEJF2II8/Krajbich et al. - 2010 - Visual fixations and the computation and compariso.pdf;/home/maarten/Zotero/storage/XIKKCRVU/nn.html}
}

@article{krueger_strategies_2017,
  title = {Strategies for Exploration in the Domain of Losses},
  author = {Krueger, P.M. and Wilson, R.C. and Cohen, J.D.},
  year = {2017},
  journal = {Judgment and Decision Making},
  volume = {12},
  number = {2},
  pages = {104--117},
  issn = {1930-2975},
  abstract = {Many decisions in everyday life involve a choice between exploring options that are currently unknown and exploiting options that are already known to be rewarding. Previous work has suggested that humans solve such ``explore-exploit'' dilemmas using a mixture of two strategies: directed exploration, in which information seeking drives exploration by choice, and random exploration, in which behavioral variability drives exploration by chance. One limitation of this previous work was that, like most studies on explore-exploit decision making, it focused exclusively on the domain of gains, where the goal was to maximize reward. In many real-world decisions, however, the goal is to minimize losses and it is well known from Prospect Theory that behavior can be quite different in this domain. In this study, we compared explore-exploit behavior of human subjects under conditions of gain and loss. We found that people use both directed and random exploration regardless of whether they are exploring to maximize gains or minimize losses and that there is quantitative agreement between the exploration parameters across domains. Our results also revealed an overall bias towards the more uncertain option in the domain of losses. While this bias towards uncertainty was qualitatively consistent with the predictions of Prospect Theory, quantitatively we found that the bias was better described by a Bayesian account, in which subjects had a prior that was optimistic for losses and pessimistic for gains. Taken together, our results suggest that explore-exploit decisions are driven by three independent processes: directed and random exploration, and a baseline uncertainty seeking that is driven by a prior. \textcopyright{} 2017, Society for Judgment and Decision making. All rights reserved.},
  langid = {english},
  keywords = {Decision noise,Explore-exploit,Information,Loss aversion,Reinforcement learning,Risk seeking,Uncertainty},
  file = {/home/maarten/Zotero/storage/ZSSYUHFB/Krueger et al. - 2017 - Strategies for exploration in the domain of losses.pdf;/home/maarten/Zotero/storage/TRA6USJZ/display.html}
}

@book{kuhn1970structure,
  title = {The Structure of Scientific Revolutions},
  author = {Kuhn, T.S.},
  editor = {Neurath, O.},
  year = {1970},
  series = {International {{Encyclopedia}} of {{Unified Science}}},
  edition = {Second},
  publisher = {{University of Chicago Press}},
  isbn = {978-0-226-45803-8},
  lccn = {79107472}
}

@article{lee_psychological_2011,
  title = {Psychological Models of Human and Optimal Performance in Bandit Problems},
  author = {Lee, Michael D. and Zhang, Shunan and Munro, Miles and Steyvers, Mark},
  year = {2011},
  month = jun,
  journal = {Cognitive Systems Research},
  series = {The 9th {{International Conference}} on {{Cognitive Modeling}}. {{Manchester}}, {{UK}}, {{July}} 2009},
  volume = {12},
  number = {2},
  pages = {164--174},
  issn = {1389-0417},
  doi = {10.1016/j.cogsys.2010.07.007},
  abstract = {In bandit problems, a decision-maker must choose between a set of alternatives, each of which has a fixed but unknown rate of reward, to maximize their total number of rewards over a sequence of trials. Performing well in these problems requires balancing the need to search for highly-rewarding alternatives, with the need to capitalize on those alternatives already known to be reasonably good. Consistent with this motivation, we develop a new psychological model that relies on switching between latent exploration and exploitation states. We test the model over a range of two-alternative bandit problems, against both human and optimal decision-making data, comparing it to benchmark models from the reinforcement learning literature. By making inferences about the latent states from optimal decision-making behavior, we characterize how people should switch between exploration and exploitation. By making inferences from human data, we begin to characterize how people actually do switch. We discuss the implications of these findings for understanding and measuring the competing demands of exploration and exploitation in sequential decision-making.},
  langid = {english},
  keywords = {Bandit problem,Exploration versus exploitation,Heuristic models,Latent state models,Reinforcement learning},
  file = {/home/maarten/Zotero/storage/ZKWCJ2FL/Lee et al. - 2011 - Psychological models of human and optimal performa.pdf;/home/maarten/Zotero/storage/F2FZCU78/S1389041710000550.html}
}

@article{leong_dynamic_2017,
  title = {Dynamic {{Interaction}} between {{Reinforcement Learning}} and {{Attention}} in {{Multidimensional Environments}}},
  author = {Leong, Yuan Chang and Radulescu, Angela and Daniel, Reka and DeWoskin, Vivian and Niv, Yael},
  year = {2017},
  month = jan,
  journal = {Neuron},
  volume = {93},
  number = {2},
  pages = {451--463},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2016.12.040},
  abstract = {Little is known about the relationship between attention and learning during decision making. Using eye tracking and multivariate pattern analysis of~fMRI data, we measured participants' dimensional attention as they performed a trial-and-error learning task in which only one of three stimulus dimensions was relevant for reward at any given time. Analysis of participants' choices revealed that attention biased both value computation during choice and value update during learning. Value signals in the ventromedial prefrontal cortex and prediction errors in the striatum were similarly biased by attention. In turn, participants' focus of attention was dynamically modulated by ongoing learning. Attentional switches across dimensions correlated with activity in a frontoparietal attention network, which showed enhanced connectivity with the ventromedial prefrontal cortex between switches. Our results suggest a bidirectional interaction between attention and learning: attention constrains learning to relevant dimensions of the environment, while we learn what to attend to via trial and error.},
  langid = {english},
  keywords = {attention,computational modeling,decision making,fMRI,MVPA,prediction error,reinforcement learning,striatum,value,vmPFC},
  file = {/home/maarten/Zotero/storage/SGZQHYMG/Leong et al. - 2017 - Dynamic Interaction between Reinforcement Learning.pdf;/home/maarten/Zotero/storage/ER2T228E/S089662731631039X.html}
}

@article{leroy_knight_1987-1,
  title = {Knight on {{Risk}} and {{Uncertainty}}},
  author = {LeRoy, Stephen F. and Singell,, Larry D.},
  year = {1987},
  month = apr,
  journal = {Journal of Political Economy},
  volume = {95},
  number = {2},
  pages = {394--406},
  issn = {0022-3808, 1537-534X},
  doi = {10.1086/261461},
  langid = {english},
  file = {/home/maarten/Zotero/storage/B6YW6WTX/LeRoy and Singell, - 1987 - Knight on Risk and Uncertainty.pdf}
}

@article{lieshout_curiosity_2021,
  title = {Curiosity or Savouring? {{Information}} Seeking Is Modulated by Both Uncertainty and Valence},
  shorttitle = {Curiosity or Savouring?},
  author = {van Lieshout, Lieke L. F. and Traast, Iris J. and de Lange, Floris P. and Cools, Roshan},
  year = {2021},
  month = sep,
  journal = {PLOS ONE},
  volume = {16},
  number = {9},
  pages = {e0257011},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0257011},
  abstract = {Curiosity is pervasive in our everyday lives, but we know little about the factors that contribute to this drive. In the current study, we assessed whether curiosity about uncertain outcomes is modulated by the valence of the information, i.e. whether the information is good or bad news. Using a lottery task in which outcome uncertainty, expected value and outcome valence (gain versus loss) were manipulated independently, we found that curiosity is overall higher for gains compared with losses and that curiosity increased with increasing outcome uncertainty for both gains and losses. These effects of uncertainty and valence did not interact, indicating that the motivation to reduce uncertainty and the motivation to maximize positive information represent separate, independent drives.},
  langid = {english},
  keywords = {Analysis of variance,Behavior,Binomials,Data visualization,Experimental economics,Functional magnetic resonance imaging,Medical risk factors,Payment},
  file = {/home/maarten/Zotero/storage/HCWFRGNZ/Lieshout et al. - 2021 - Curiosity or savouring Information seeking is mod.pdf;/home/maarten/Zotero/storage/BYDLRDCC/article.html}
}

@article{mathys_bayesian_2011,
  title = {A {{Bayesian Foundation}} for {{Individual Learning Under Uncertainty}}},
  author = {Mathys, Christoph and Daunizeau, Jean and Friston, Karl and Stephan, Klaas},
  year = {2011},
  journal = {Frontiers in Human Neuroscience},
  volume = {5},
  pages = {39},
  issn = {1662-5161},
  doi = {10.3389/fnhum.2011.00039},
  abstract = {Computational learning models are critical for understanding mechanisms of adaptive behavior. However, the two major current frameworks, reinforcement learning (RL) and Bayesian learning, both have certain limitations. For example, many Bayesian models are agnostic of inter-individual variability and involve complicated integrals, making online learning difficult. Here, we introduce a generic hierarchical Bayesian framework for individual learning under multiple forms of uncertainty (e.g., environmental volatility and perceptual uncertainty). The model assumes Gaussian random walks of states at all but the first level, with the step size determined by the next highest level. The coupling between levels is controlled by parameters that shape the influence of uncertainty on learning in a subject-specific fashion. Using variational Bayes under a mean-field approximation and a novel approximation to the posterior energy function, we derive trial-by-trial update equations which (i) are analytical and extremely efficient, enabling real-time learning, (ii) have a natural interpretation in terms of RL, and (iii) contain parameters representing processes which play a key role in current theories of learning, e.g., precision-weighting of prediction error. These parameters allow for the expression of individual differences in learning and may relate to specific neuromodulatory mechanisms in the brain. Our model is very general: it can deal with both discrete and continuous states and equally accounts for deterministic and probabilistic relations between environmental events and perceptual states (i.e., situations with and without perceptual uncertainty). These properties are illustrated by simulations and analyses of empirical time series. Overall, our framework provides a novel foundation for understanding normal and pathological learning that contextualizes RL within a generic Bayesian scheme and thus connects it to principles of optimality from probability theory.},
  file = {/home/maarten/Zotero/storage/4M9XBB2P/Mathys et al. - 2011 - A Bayesian Foundation for Individual Learning Unde.pdf}
}

@article{may_optimistic_2012,
  title = {Optimistic {{Bayesian}} Sampling in Contextual-Bandit Problems},
  author = {May, Benedict C. and Korda, Nathan and Lee, Anthony and Leslie, David S.},
  year = {2012},
  journal = {Journal of Machine Learning Research},
  volume = {13},
  pages = {2069--2106},
  file = {/home/maarten/Zotero/storage/DU7AH5FI/May et al. - 2012 - Optimistic Bayesian sampling in contextual-bandit .pdf}
}

@article{mcdougle_modeling_2021,
  title = {Modeling the Influence of Working Memory, Reinforcement, and Action Uncertainty on Reaction Time and Choice during Instrumental Learning},
  author = {McDougle, Samuel D. and Collins, Anne G. E.},
  year = {2021},
  month = feb,
  journal = {Psychonomic Bulletin \& Review},
  volume = {28},
  number = {1},
  pages = {20--39},
  issn = {1531-5320},
  doi = {10.3758/s13423-020-01774-z},
  abstract = {What determines the speed of our decisions? Various models of decision-making have focused on perceptual evidence, past experience, and task complexity as important factors determining the degree of deliberation needed for a decision. Here, we build on a sequential sampling decision-making framework to develop a new model that captures a range of reaction time (RT) effects by accounting for both working memory and instrumental learning processes. The model captures choices and RTs at various stages of learning, and in learning environments with varying complexity. Moreover, the model generalizes from tasks with deterministic reward contingencies to probabilistic ones. The model succeeds in part by incorporating prior uncertainty over actions when modeling RT. This straightforward process model provides a parsimonious account of decision dynamics during instrumental learning and makes unique predictions about internal representations of action values.},
  langid = {english},
  file = {/home/maarten/Zotero/storage/R7MGVI26/McDougle and Collins - 2021 - Modeling the influence of working memory, reinforc.pdf}
}

@article{mehlhorn_unpacking_2015,
  title = {Unpacking the Exploration\textendash Exploitation Tradeoff: {{A}} Synthesis of Human and Animal Literatures},
  shorttitle = {Unpacking the Exploration\textendash Exploitation Tradeoff},
  author = {Mehlhorn, Katja and Newell, Ben R. and Todd, Peter M. and Lee, Michael D. and Morgan, Kate and Braithwaite, Victoria A. and Hausmann, Daniel and Fiedler, Klaus and Gonzalez, Cleotilde},
  year = {2015},
  journal = {Decision},
  volume = {2},
  number = {3},
  pages = {191--215},
  publisher = {{Educational Publishing Foundation}},
  address = {{US}},
  issn = {2325-9973},
  doi = {10.1037/dec0000033},
  abstract = {Many decisions in the lives of animals and humans require a fine balance between the exploration of different options and the exploitation of their rewards. Do you buy the advertised car, or do you test drive different models? Do you continue feeding from the current patch of flowers, or do you fly off to another one? Do you marry your current partner, or try your luck with someone else? The balance required in these situations is commonly referred to as the exploration\textendash exploitation tradeoff. It features prominently in a wide range of research traditions, including learning, foraging, and decision making literatures. Here, we integrate findings from these and other often-isolated literatures in order to gain a better understanding of the possible tradeoffs between exploration and exploitation, and we propose new theoretical insights that might guide future research. Specifically, we explore how potential tradeoffs depend on (a) the conceptualization of exploration and exploitation; (b) the influencing environmental, social, and individual factors; (c) the scale at which exploration and exploitation are considered; (d) the relationship and types of transitions between the 2 behaviors; and (e) the goals of the decision maker. We conclude that exploration and exploitation are best conceptualized as points on a continuum, and that the extent to which an agent's behavior can be interpreted as exploratory or exploitative depends upon the level of abstraction at which it is considered. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Animal Foraging Behavior,Animals,Comparative Psychology,Decision Making,Learning,Rewards,Theories},
  file = {/home/maarten/Zotero/storage/ND5ISWLK/Mehlhorn et al. - 2015 - Unpacking the explorationâ€“exploitation tradeoff A.pdf;/home/maarten/Zotero/storage/D2F5CAC8/2015-14537-001.html}
}

@article{monosov_how_2020,
  title = {How {{Outcome Uncertainty Mediates Attention}}, {{Learning}}, and {{Decision-Making}}},
  author = {Monosov, Ilya E.},
  year = {2020},
  month = oct,
  journal = {Trends in Neurosciences},
  volume = {43},
  number = {10},
  pages = {795--809},
  issn = {0166-2236},
  doi = {10.1016/j.tins.2020.06.009},
  abstract = {Animals and humans evolved sophisticated nervous systems that endowed them with the ability to form internal-models or beliefs and make predictions about the future to survive and flourish in a world in which future outcomes are often uncertain. Crucial to this capacity is the ability to adjust behavioral and learning policies in response to the level of uncertainty. Until recently, the neuronal mechanisms that could underlie such uncertainty-guided control have been largely unknown. In this review, I discuss newly discovered neuronal circuits in primates that represent uncertainty about future rewards and propose how they guide information-seeking, attention, decision-making, and learning to help us survive in an uncertain world. Lastly, I discuss the possible relevance of these findings to learning in artificial systems.},
  langid = {english},
  keywords = {artificial intelligence,basal-forebrain,basal-ganglia,cingulate,information-seeking},
  file = {/home/maarten/Zotero/storage/UCJ845XY/1-s2.0-S016622362030151X-main.pdf;/home/maarten/Zotero/storage/PIX27APJ/S016622362030151X.html}
}

@article{nassar_approximately_2010,
  title = {An {{Approximately Bayesian Delta-Rule Model Explains}} the {{Dynamics}} of {{Belief Updating}} in a {{Changing Environment}}},
  author = {Nassar, Matthew R. and Wilson, Robert C. and Heasly, Benjamin and Gold, Joshua I.},
  year = {2010},
  month = sep,
  journal = {The Journal of Neuroscience},
  volume = {30},
  number = {37},
  pages = {12366--12378},
  issn = {0270-6474},
  doi = {10.1523/JNEUROSCI.0822-10.2010},
  abstract = {Maintaining appropriate beliefs about variables needed for effective decision making can be difficult in a dynamic environment. One key issue is the amount of influence that unexpected outcomes should have on existing beliefs. In general, outcomes that are unexpected because of a fundamental change in the environment should carry more influence than outcomes that are unexpected because of persistent environmental stochasticity. Here we use a novel task to characterize how well human subjects follow these principles under a range of conditions. We show that the influence of an outcome depends on both the error made in predicting that outcome and the number of similar outcomes experienced previously. We also show that the exact nature of these tendencies varies considerably across subjects. Finally, we show that these patterns of behavior are consistent with a computationally simple reduction of an ideal-observer model. The model adjusts the influence of newly experienced outcomes according to ongoing estimates of uncertainty and the probability of a fundamental change in the process by which outcomes are generated. A prior that quantifies the expected frequency of such environmental changes accounts for individual variability, including a positive relationship between subjective certainty and the degree to which new information influences existing beliefs. The results suggest that the brain adaptively regulates the influence of decision outcomes on existing beliefs using straightforward updating rules that take into account both recent outcomes and prior expectations about higher-order environmental structure.},
  pmcid = {PMC2945906},
  pmid = {20844132},
  file = {/home/maarten/Zotero/storage/7WGGAMXZ/An Approximately Bayesian Delta-Rule Model Explain.pdf}
}

@article{navarro_learning_2016,
  title = {Learning and Choosing in an Uncertain World: {{An}} Investigation of the Explore\textendash Exploit Dilemma in Static and Dynamic Environments},
  shorttitle = {Learning and Choosing in an Uncertain World},
  author = {Navarro, Daniel J. and Newell, Ben R. and Schulze, Christin},
  year = {2016},
  month = mar,
  journal = {Cognitive Psychology},
  volume = {85},
  pages = {43--77},
  issn = {0010-0285},
  doi = {10.1016/j.cogpsych.2016.01.001},
  abstract = {How do people solve the explore\textendash exploit trade-off in a changing environment? In this paper we present experimental evidence from an ``observe or bet'' task, in which people have to determine when to engage in information-seeking behavior and when to switch to reward-taking actions. In particular we focus on the comparison between people's behavior in a changing environment and their behavior in an unchanging one. Our experimental work is motivated by rational analysis of the problem that makes strong predictions about information search and reward seeking in static and changeable environments. Our results show a striking agreement between human behavior and the optimal policy, but also highlight a number of systematic differences. In particular, we find that while people often employ suboptimal strategies the first time they encounter the learning problem, most people are able to approximate the correct strategy after minimal experience. In order to describe both the manner in which people's choices are similar to but slightly different from an optimal standard, we introduce four process models for the observe or bet task and evaluate them as potential theories of human behavior.},
  langid = {english},
  keywords = {Decision making,Decisions from experience,Dynamic environments,Exploreâ€“exploit dilemma},
  file = {/home/maarten/Zotero/storage/FYCBWAKH/Navarro et al. - 2016 - Learning and choosing in an uncertain world An in.pdf;/home/maarten/Zotero/storage/6RF655AB/S0010028516000025.html}
}

@article{niv_reinforcement_2015,
  title = {Reinforcement {{Learning}} in {{Multidimensional Environments Relies}} on {{Attention Mechanisms}}},
  author = {Niv, Yael and Daniel, Reka and Geana, Andra and Gershman, Samuel J. and Leong, Yuan Chang and Radulescu, Angela and Wilson, Robert C.},
  year = {2015},
  month = may,
  journal = {Journal of Neuroscience},
  volume = {35},
  number = {21},
  pages = {8145--8157},
  publisher = {{Society for Neuroscience}},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2978-14.2015},
  abstract = {In recent years, ideas from the computational field of reinforcement learning have revolutionized the study of learning in the brain, famously providing new, precise theories of how dopamine affects learning in the basal ganglia. However, reinforcement learning algorithms are notorious for not scaling well to multidimensional environments, as is required for real-world learning. We hypothesized that the brain naturally reduces the dimensionality of real-world problems to only those dimensions that are relevant to predicting reward, and conducted an experiment to assess by what algorithms and with what neural mechanisms this ``representation learning'' process is realized in humans. Our results suggest that a bilateral attentional control network comprising the intraparietal sulcus, precuneus, and dorsolateral prefrontal cortex is involved in selecting what dimensions are relevant to the task at hand, effectively updating the task representation through trial and error. In this way, cortical attention mechanisms interact with learning in the basal ganglia to solve the ``curse of dimensionality'' in reinforcement learning.},
  chapter = {Articles},
  copyright = {Copyright \textcopyright{} 2015 the authors 0270-6474/15/358145-13\$15.00/0},
  langid = {english},
  pmid = {26019331},
  keywords = {attention,fMRI,frontoparietal network,model comparison,reinforcement learning,representation learning},
  file = {/home/maarten/Zotero/storage/HI3HXHL6/Niv et al. - 2015 - Reinforcement Learning in Multidimensional Environ.pdf;/home/maarten/Zotero/storage/X8PK4X49/8145.html}
}

@article{payzan-lenestour_not_2012,
  title = {Do Not {{Bet}} on the {{Unknown Versus Try}} to {{Find Out More}}: {{Estimation Uncertainty}} and ``{{Unexpected Uncertainty}}'' {{Both Modulate Exploration}}},
  shorttitle = {Do Not {{Bet}} on the {{Unknown Versus Try}} to {{Find Out More}}},
  author = {{Payzan-LeNestour}, Elise and Bossaerts, Peter},
  year = {2012},
  journal = {Frontiers in Neuroscience},
  volume = {6},
  pages = {150},
  issn = {1662-453X},
  doi = {10.3389/fnins.2012.00150},
  abstract = {Little is known about how humans solve the exploitation/exploration trade-off. In particular, the evidence for uncertainty-driven exploration is mixed. The current study proposes a novel hypothesis of exploration that helps reconcile prior findings that may seem contradictory at first. According to this hypothesis, uncertainty-driven exploration involves a dilemma between two motives: (i) to speed up learning about the unknown, which may beget novel reward opportunities; (ii) to avoid the unknown because it is potentially dangerous. We provide evidence for our hypothesis using both behavioral and simulated data, and briefly point to recent evidence that the brain differentiates between these two motives.},
  file = {/home/maarten/Zotero/storage/QYNA6KQS/Payzan-LeNestour and Bossaerts - 2012 - Do not Bet on the Unknown Versus Try to Find Out M.pdf}
}

@article{payzan-lenestour_risk_2011,
  title = {Risk, {{Unexpected Uncertainty}}, and {{Estimation Uncertainty}}: {{Bayesian Learning}} in {{Unstable Settings}}},
  shorttitle = {Risk, {{Unexpected Uncertainty}}, and {{Estimation Uncertainty}}},
  author = {{Payzan-LeNestour}, Elise and Bossaerts, Peter},
  year = {2011},
  month = jan,
  journal = {PLOS Computational Biology},
  volume = {7},
  number = {1},
  pages = {e1001048},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1001048},
  abstract = {Recently, evidence has emerged that humans approach learning using Bayesian updating rather than (model-free) reinforcement algorithms in a six-arm restless bandit problem. Here, we investigate what this implies for human appreciation of uncertainty. In our task, a Bayesian learner distinguishes three equally salient levels of uncertainty. First, the Bayesian perceives irreducible uncertainty or risk: even knowing the payoff probabilities of a given arm, the outcome remains uncertain. Second, there is (parameter) estimation uncertainty or ambiguity: payoff probabilities are unknown and need to be estimated. Third, the outcome probabilities of the arms change: the sudden jumps are referred to as unexpected uncertainty. We document how the three levels of uncertainty evolved during the course of our experiment and how it affected the learning rate. We then zoom in on estimation uncertainty, which has been suggested to be a driving force in exploration, in spite of evidence of widespread aversion to ambiguity. Our data corroborate the latter. We discuss neural evidence that foreshadowed the ability of humans to distinguish between the three levels of uncertainty. Finally, we investigate the boundaries of human capacity to implement Bayesian learning. We repeat the experiment with different instructions, reflecting varying levels of structural uncertainty. Under this fourth notion of uncertainty, choices were no better explained by Bayesian updating than by (model-free) reinforcement learning. Exit questionnaires revealed that participants remained unaware of the presence of unexpected uncertainty and failed to acquire the right model with which to implement Bayesian updating.},
  langid = {english},
  keywords = {Algorithms,Decision making,Entropy,Evolutionary rate,Human learning,Learning,Probability distribution,Statistical models},
  file = {/home/maarten/Zotero/storage/PWRUYRZJ/Payzan-LeNestour and Bossaerts - 2011 - Risk, Unexpected Uncertainty, and Estimation Uncer.pdf;/home/maarten/Zotero/storage/YBS7YIZP/article.html}
}

@article{pearce_model_1980,
  title = {A Model for {{Pavlovian}} Learning: {{Variations}} in the Effectiveness of Conditioned but Not of Unconditioned Stimuli},
  shorttitle = {A Model for {{Pavlovian}} Learning},
  author = {Pearce, John M. and Hall, Geoffrey},
  year = {1980},
  journal = {Psychological Review},
  volume = {87},
  number = {6},
  pages = {532--552},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1471},
  doi = {10.1037/0033-295X.87.6.532},
  abstract = {Part 1 of this discussion summarizes several formal models of exicitatory classical conditioning. It is suggested that a central problem for all of them is the explanation of cases in which learning does not occur in spite of the fact that the CS is a signal for the reinforcer. A new model is proposed that deals with this problem by specifying that certain procedures cause a CS to lose effectiveness; in particular, it is argued that a CS will lose associability when its consequences are accurately predicted. In contrast to other current models, the effectiveness of the reinforcer remains constant throughout conditioning. Part 2 presents a reformulation of the nature of the learning produced by inhibitory-conditioning procedures and a discussion of the way in which such learning can be accommodated within the model outlined for excitatory learning. (47 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Classical Conditioning,Models},
  file = {/home/maarten/Zotero/storage/IC3LH3VX/Pearce and Hall - 1980 - A model for Pavlovian learning Variations in the .pdf;/home/maarten/Zotero/storage/NFB4ZMW8/1981-02676-001.html}
}

@article{pedersen_drift_2017,
  title = {The Drift Diffusion Model as the Choice Rule in Reinforcement Learning},
  author = {Pedersen, Mads Lund and Frank, Michael J. and Biele, Guido},
  year = {2017},
  month = aug,
  journal = {Psychonomic Bulletin \& Review},
  volume = {24},
  number = {4},
  pages = {1234--1251},
  issn = {1531-5320},
  doi = {10.3758/s13423-016-1199-y},
  abstract = {Current reinforcement-learning models often assume simplified decision processes that do not fully reflect the dynamic complexities of choice processes. Conversely, sequential-sampling models of decision making account for both choice accuracy and response time, but assume that decisions are based on static decision values. To combine these two computational models of decision making and learning, we implemented reinforcement-learning models in which the drift diffusion model describes the choice process, thereby capturing both within- and across-trial dynamics. To exemplify the utility of this approach, we quantitatively fit data from a common reinforcement-learning paradigm using hierarchical Bayesian parameter estimation, and compared model variants to determine whether they could capture the effects of stimulant medication in adult patients with attention-deficit hyperactivity disorder (ADHD). The model with the best relative fit provided a good description of the learning process, choices, and response times. A parameter recovery experiment showed that the hierarchical Bayesian modeling approach enabled accurate estimation of the model parameters. The model approach described here, using simultaneous estimation of reinforcement-learning and drift diffusion model parameters, shows promise for revealing new insights into the cognitive and neural mechanisms of learning and decision making, as well as the alteration of such processes in clinical groups.},
  langid = {english},
  file = {/home/maarten/Zotero/storage/2BZVADCB/Pedersen et al. - 2017 - The drift diffusion model as the choice rule in re.pdf}
}

@article{rich_exploratory_2018,
  title = {Exploratory Choice Reflects the Future Value of Information},
  author = {Rich, Alexander S. and Gureckis, Todd M.},
  year = {2018},
  journal = {Decision},
  volume = {5},
  number = {3},
  pages = {177--192},
  publisher = {{Educational Publishing Foundation}},
  address = {{US}},
  issn = {2325-9973},
  doi = {10.1037/dec0000074},
  abstract = {The tension between exploration and exploitation is a primary challenge in decision making under uncertainty. Optimal models of choice prescribe that individuals resolve this tension by evaluating how information gained from their choices will improve future choices. However, research in behavioral economics and psychology has yielded conflicting evidence about whether people consider the future during exploratory choice, particularly in complex, uncertain environments. Adding to the empirical evidence on this issue, we examine exploratory decision making in a novel approach-avoid paradigm. In the first set of experiments we find that people parametrically increase their exploration when the expected number of future encounters with a prospect is larger. In the second we demonstrate that when the number of future encounters is unknown, as is often the case in everyday life, people are sensitive to the relative frequency of future encounters with a prospect. Our experiments show that people adaptively utilize information about the future when deciding to explore, a tendency that may shape decisions across several real-world domains. (PsycINFO Database Record (c) 2018 APA, all rights reserved)},
  keywords = {Choice Behavior,Decision Making,Future,Information,Learning},
  file = {/home/maarten/Zotero/storage/ILUGFJDA/Rich and Gureckis - 2018 - Exploratory choice reflects the future value of in.pdf;/home/maarten/Zotero/storage/K8AXZ3ZL/Exploratory Choice Reflects the Future Value of Information.pdf;/home/maarten/Zotero/storage/7S48XKUT/2017-00520-001.html}
}

@article{roca_ambiguity_2006,
  title = {Ambiguity Seeking as a Result of the Status Quo Bias},
  author = {Roca, Merc{\`e} and Hogarth, Robin M. and Maule, A. John},
  year = {2006},
  month = may,
  journal = {Journal of Risk and Uncertainty},
  volume = {32},
  number = {3},
  pages = {175--194},
  issn = {1573-0476},
  doi = {10.1007/s11166-006-9518-8},
  abstract = {Several factors affect attitudes toward ambiguity. What happens, however, when people are asked to exchange an ambiguous alternative in their possession for an unambiguous one? We present three experiments in which individuals preferred to retain the former. This status quo bias emerged both within- and between-subjects, with and without incentives, with different outcome distributions, and with endowments determined by both the experimenter and the participants themselves. Findings emphasize the need to account for the frames of reference under which evaluations of probabilistic information take place as well as modifications that should be incorporated into descriptive models of decision making.},
  langid = {english},
  file = {/home/maarten/Zotero/storage/EMSLMALH/Roca et al. - 2006 - Ambiguity seeking as a result of the status quo bi.pdf}
}

@article{roesch_surprise_2012,
  title = {Surprise! {{Neural}} Correlates of {{Pearce}}\textendash{{Hall}} and {{Rescorla}}\textendash{{Wagner}} Coexist within the Brain},
  author = {Roesch, Matthew R. and Esber, Guillem R. and Li, Jian and Daw, Nathaniel D. and Schoenbaum, Geoffrey},
  year = {2012},
  journal = {European Journal of Neuroscience},
  volume = {35},
  number = {7},
  pages = {1190--1200},
  issn = {1460-9568},
  doi = {10.1111/j.1460-9568.2011.07986.x},
  abstract = {Learning theory and computational accounts suggest that learning depends on errors in outcome prediction as well as changes in processing of or attention to events. These divergent ideas are captured by models, such as Rescorla\textendash Wagner (RW) and temporal difference (TD) learning on the one hand, which emphasize errors as directly driving changes in associative strength, vs. models such as Pearce\textendash Hall (PH) and more recent variants on the other hand, which propose that errors promote changes in associative strength by modulating attention and processing of events. Numerous studies have shown that phasic firing of midbrain dopamine (DA) neurons carries a signed error signal consistent with RW or TD learning theories, and recently we have shown that this signal can be dissociated from attentional correlates in the basolateral amygdala and anterior cingulate. Here we will review these data along with new evidence: (i) implicating habenula and striatal regions in supporting error signaling in midbrain DA neurons; and (ii) suggesting that the central nucleus of the amygdala and prefrontal regions process the amygdalar attentional signal. However, while the neural instantiations of the RW and PH signals are dissociable and complementary, they may be linked. Any linkage would have implications for understanding why one signal dominates learning in some situations and not others, and also for appreciating the potential impact on learning of neuropathological conditions involving altered DA or amygdalar function, such as schizophrenia, addiction or anxiety disorders.},
  langid = {english},
  keywords = {amygdala,attention,dopamine,learning,prediction error},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1460-9568.2011.07986.x},
  file = {/home/maarten/Zotero/storage/TQ67K5T6/Roesch et al. - 2012 - Surprise! Neural correlates of Pearceâ€“Hall and Res.pdf;/home/maarten/Zotero/storage/934QFGN4/j.1460-9568.2011.07986.html}
}

@article{samuelson_status_1988,
  title = {Status Quo Bias in Decision Making},
  author = {Samuelson, William and Zeckhauser, Richard},
  year = {1988},
  month = mar,
  journal = {Journal of Risk and Uncertainty},
  volume = {1},
  number = {1},
  pages = {7--59},
  publisher = {{Kluwer Academic Publishers}},
  issn = {1573-0476},
  doi = {10.1007/BF00055564},
  abstract = {Most real decisions, unlike those of economics texts, have a status quo alternative\textemdash that is, doing nothing or maintaining one's current or previous decision. A series of decision-making experiments shows that individuals disproportionately stick with the status quo. Data on the selections of health plans and retirement programs by faculty members reveal that the status quo bias is substantial in important real decisions. Economics, psychology, and decision theory provide possible explanations for this bias. Applications are discussed ranging from marketing techniques, to industrial organization, to the advance of science.},
  copyright = {1988 Kluwer Academic Publishers},
  langid = {english},
  file = {/home/maarten/Zotero/storage/S3Q4KZGD/Samuelson and Zeckhauser - 1988 - Status quo bias in decision making.pdf;/home/maarten/Zotero/storage/3LQ37334/10.html}
}

@article{sang_simple_2020,
  title = {Simple {{Threshold Rules Solve Explore}}/{{Exploit Trade-offs}} in a {{Resource Accumulation Search Task}}},
  author = {Sang, Ke and Todd, Peter M. and Goldstone, Robert L. and Hills, Thomas T.},
  year = {2020},
  journal = {Cognitive Science},
  volume = {44},
  number = {2},
  pages = {e12817},
  issn = {1551-6709},
  doi = {10.1111/cogs.12817},
  abstract = {How, and how well, do people switch between exploration and exploitation to search for and accumulate resources? We study the decision processes underlying such exploration/exploitation trade-offs using a novel card selection task that captures the common situation of searching among multiple resources (e.g., jobs) that can be exploited without depleting. With experience, participants learn to switch appropriately between exploration and exploitation and approach optimal performance. We model participants' behavior on this task with random, threshold, and sampling strategies, and find that a linear decreasing threshold rule best fits participants' results. Further evidence that participants use decreasing threshold-based strategies comes from reaction time differences between exploration and exploitation; however, participants themselves report non-decreasing thresholds. Decreasing threshold strategies that ``front-load'' exploration and switch quickly to exploitation are particularly effective in resource accumulation tasks, in contrast to optimal stopping problems like the Secretary Problem requiring longer exploration.},
  langid = {english},
  keywords = {exploitation,Exploration,Model comparison,Optimal search,Resource patches,Secretary Problem,Threshold strategy},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cogs.12817},
  file = {/home/maarten/Zotero/storage/6EIEK9FV/Sang et al. - 2020 - Simple Threshold Rules Solve ExploreExploit Trade.pdf;/home/maarten/Zotero/storage/C88V38PD/cogs.html}
}

@article{schulz_algorithmic_2019,
  title = {The Algorithmic Architecture of Exploration in the Human Brain},
  author = {Schulz, Eric and Gershman, Samuel J.},
  year = {2019},
  month = apr,
  journal = {Current Opinion in Neurobiology},
  series = {Machine {{Learning}}, {{Big Data}}, and {{Neuroscience}}},
  volume = {55},
  pages = {7--14},
  issn = {0959-4388},
  doi = {10.1016/j.conb.2018.11.003},
  abstract = {Balancing exploration and exploitation is one of the central problems in reinforcement learning. We review recent studies that have identified multiple algorithmic strategies underlying exploration. In particular, humans use a combination of random and uncertainty-directed exploration strategies, which rely on different brain systems, have different developmental trajectories, and are sensitive to different task manipulations. Humans are also able to exploit sophisticated structural knowledge to aid their exploration, such as information about correlations between options. New computational models, drawing inspiration from machine learning, have begun to formalize these ideas and offer new ways to understand the neural basis of reinforcement learning.},
  langid = {english},
  file = {/home/maarten/Zotero/storage/SWQYWMTD/Schulz and Gershman - 2019 - The algorithmic architecture of exploration in the.pdf;/home/maarten/Zotero/storage/CKLU58CN/S0959438818300904.html}
}

@article{schulz_generalization_2018,
  title = {Generalization and {{Search}} in {{Risky Environments}}},
  author = {Schulz, Eric and Wu, Charley M. and Huys, Quentin J. M. and Krause, Andreas and Speekenbrink, Maarten},
  year = {2018},
  journal = {Cognitive Science},
  volume = {42},
  number = {8},
  pages = {2592--2620},
  issn = {1551-6709},
  doi = {10.1111/cogs.12695},
  abstract = {How do people pursue rewards in risky environments, where some outcomes should be avoided at all costs? We investigate how participant search for spatially correlated rewards in scenarios where one must avoid sampling rewards below a given threshold. This requires not only the balancing of exploration and exploitation, but also reasoning about how to avoid potentially risky areas of the search space. Within risky versions of the spatially correlated multi-armed bandit task, we show that participants' behavior is aligned well with a Gaussian process function learning algorithm, which chooses points based on a safe optimization routine. Moreover, using leave-one-block-out cross-validation, we find that participants adapt their sampling behavior to the riskiness of the task, although the underlying function learning mechanism remains relatively unchanged. These results show that participants can adapt their search behavior to the adversity of the environment and enrich our understanding of adaptive behavior in the face of risk and uncertainty.},
  langid = {english},
  keywords = {Explorationâ€“Exploitation,Function learning,Generalization,Risky choices},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cogs.12695},
  file = {/home/maarten/Zotero/storage/W5ZL6DIQ/Schulz et al. - 2018 - Generalization and Search in Risky Environments.pdf;/home/maarten/Zotero/storage/TZE9XYN3/cogs.html}
}

@article{schulz_putting_2018,
  title = {Putting Bandits into Context: {{How}} Function Learning Supports Decision Making},
  shorttitle = {Putting Bandits into Context},
  author = {Schulz, Eric and Konstantinidis, Emmanouil and Speekenbrink, Maarten},
  year = {2018},
  journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume = {44},
  number = {6},
  pages = {927--943},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1285},
  doi = {10.1037/xlm0000463},
  abstract = {The authors introduce the contextual multi-armed bandit task as a framework to investigate learning and decision making in uncertain environments. In this novel paradigm, participants repeatedly choose between multiple options in order to maximize their rewards. The options are described by a number of contextual features which are predictive of the rewards through initially unknown functions. From their experience with choosing options and observing the consequences of their decisions, participants can learn about the functional relation between contexts and rewards and improve their decision strategy over time. In three experiments, the authors explore participants' behavior in such learning environments. They predict participants' behavior by context-blind (mean-tracking, Kalman filter) and contextual (Gaussian process and linear regression) learning approaches combined with different choice strategies. Participants are mostly able to learn about the context-reward functions and their behavior is best described by a Gaussian process learning strategy which generalizes previous experience to similar instances. In a relatively simple task with binary features, they seem to combine this learning with a probability of improvement decision strategy which focuses on alternatives that are expected to lead to an improvement upon a current favorite option. In a task with continuous features that are linearly related to the rewards, participants seem to more explicitly balance exploration and exploitation. Finally, in a difficult learning environment where the relation between features and rewards is nonlinear, some participants are again well-described by a Gaussian process learning strategy, whereas others revert to context-blind strategies. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  keywords = {Decision Making,Learning,Reinforcement},
  file = {/home/maarten/Zotero/storage/T2FUIMP3/Schulz et al. - 2018 - Putting bandits into context How function learnin.pdf;/home/maarten/Zotero/storage/L538IMZY/2017-50771-001.html}
}

@article{schulz_searching_2019,
  title = {Searching for {{Rewards Like}} a {{Child Means Less Generalization}} and {{More Directed Exploration}}},
  author = {Schulz, Eric and Wu, Charley M. and Ruggeri, Azzurra and Meder, Bj{\"o}rn},
  year = {2019},
  month = nov,
  journal = {Psychological Science},
  volume = {30},
  number = {11},
  pages = {1561--1572},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1177/0956797619863663},
  abstract = {How do children and adults differ in their search for rewards? We considered three different hypotheses that attribute developmental differences to (a) children's increased random sampling, (b) more directed exploration toward uncertain options, or (c) narrower generalization. Using a search task in which noisy rewards were spatially correlated on a grid, we compared the ability of 55 younger children (ages 7 and 8 years), 55 older children (ages 9\textendash 11 years), and 50 adults (ages 19\textendash 55 years) to successfully generalize about unobserved outcomes and balance the exploration\textendash exploitation dilemma. Our results show that children explore more eagerly than adults but obtain lower rewards. We built a predictive model of search to disentangle the unique contributions of the three hypotheses of developmental differences and found robust and recoverable parameter estimates indicating that children generalize less and rely on directed exploration more than adults. We did not, however, find reliable differences in terms of random sampling.},
  langid = {english},
  keywords = {development,explorationâ€“exploitation,generalization,multiarmed-bandit task,open data,open materials,search},
  file = {/home/maarten/Zotero/storage/JZ68YJKW/Schulz et al. - 2019 - Searching for Rewards Like a Child Means Less Gene.pdf}
}

@article{schulz_strategic_2017,
  title = {Strategic Exploration in Human Adaptive Control},
  author = {Schulz, Eric and Klenske, Edgar D. and Bramley, Neil R. and Speekenbrink, Maarten},
  year = {2017},
  month = may,
  journal = {Proceedings of the 38th Annual Meeting of the Cognitive Science Society},
  pages = {1140--1145},
  publisher = {{Cognitive Science Society}},
  address = {{Austin, TX}},
  doi = {10.1101/110486},
  abstract = {How do people explore in order to gain rewards in uncertain dynamical systems? Within a reinforcement learning paradigm, control normally involves trading off between exploration (i.e. trying out actions in order to gain more knowledge about the system) and exploitation (i.e. using current knowledge of the system to maximize reward). We study a novel control task in which participants must steer a boat on a grid, aiming to follow a path of high reward whilst learning how their actions affect the boat's position. We find that participants explore strategically yet conservatively, exploring more when mistakes are less costly and practicing actions that will be required later on.},
  chapter = {New Results},
  copyright = {\textcopyright{} 2017, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/home/maarten/Zotero/storage/ELDIISUS/Schulz et al. - 2017 - Strategic exploration in human adaptive control.pdf;/home/maarten/Zotero/storage/MT6BPPRJ/110486v2.html}
}

@article{shadlen_decision_2016,
  title = {Decision {{Making}} and {{Sequential Sampling}} from {{Memory}}},
  author = {Shadlen, Michael N. and Shohamy, Daphna},
  year = {2016},
  month = jun,
  journal = {Neuron},
  volume = {90},
  number = {5},
  pages = {927--939},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2016.04.036},
  abstract = {Decisions take time, and as a rule more difficult decisions take more time. But this only raises the question of what consumes the time. For decisions informed by a sequence of samples of evidence, the answer is straightforward: more samples are available with more time. Indeed, the speed and accuracy of such decisions are explained by the accumulation of evidence to a threshold or bound. However, the same framework seems to apply to decisions that are not obviously informed by sequences of evidence samples. Here, we proffer the hypothesis that the sequential character of such tasks involves retrieval of evidence from memory. We explore this hypothesis by focusing on value-based decisions and argue that mnemonic processes can account for regularities in choice and decision time. We speculate on the neural mechanisms that link sampling of evidence from memory to circuits that represent the accumulated evidence bearing on a choice. We propose that memory processes may contribute to a wider class of decisions that conform to the regularities of choice-reaction time predicted by the sequential sampling framework.},
  langid = {english},
  file = {/home/maarten/Zotero/storage/2DTUAAYY/Shadlen and Shohamy - 2016 - Decision Making and Sequential Sampling from Memor.pdf;/home/maarten/Zotero/storage/P6KE2M79/S0896627316301234.html}
}

@article{soltani_adaptive_2019,
  title = {Adaptive Learning under Expected and Unexpected Uncertainty},
  author = {Soltani, Alireza and Izquierdo, Alicia},
  year = {2019},
  month = oct,
  journal = {Nature Reviews Neuroscience},
  volume = {20},
  number = {10},
  pages = {635--644},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0048},
  doi = {10.1038/s41583-019-0180-y},
  abstract = {The outcome of a decision is often uncertain, and outcomes can vary over repeated decisions. Whether decision outcomes should substantially affect behaviour and learning depends on whether they are representative of a typically experienced range of outcomes or signal a change in the reward environment. Successful learning and decision-making therefore require the ability to estimate expected uncertainty (related to the variability of outcomes) and unexpected uncertainty (related to the variability of the environment). Understanding the bases and effects of these two types of uncertainty and the interactions between them \textemdash{} at the computational and the neural level \textemdash{} is crucial for understanding adaptive learning. Here, we examine computational models and experimental findings to distil computational principles and neural mechanisms for adaptive learning under uncertainty.},
  copyright = {2019 The Publisher},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Reviews Subject\_term: Amygdala;Computational neuroscience;Learning and memory;Prefrontal cortex Subject\_term\_id: amygdala;computational-neuroscience;learning-and-memory;prefrontal-cortex},
  file = {/home/maarten/Zotero/storage/IMNDKIG9/Soltani and Izquierdo - 2019 - Adaptive learning under expected and unexpected un.pdf;/home/maarten/Zotero/storage/BPLMZ5GE/s41583-019-0180-y.html}
}

@article{speekenbrink_learning_2010,
  title = {Learning in a Changing Environment},
  author = {Speekenbrink, Maarten and Shanks, David R.},
  year = {2010},
  journal = {Journal of Experimental Psychology: General},
  volume = {139},
  number = {2},
  pages = {266--298},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-2222},
  doi = {10.1037/a0018620},
  abstract = {Multiple cue probability learning studies have typically focused on stationary environments. We present 3 experiments investigating learning in changing environments. A fine-grained analysis of the learning dynamics shows that participants were responsive to both abrupt and gradual changes in cue\textendash outcome relations. We found no evidence that participants adapted to these types of change in qualitatively different ways. Also, in contrast to earlier claims that these tasks are learned implicitly, participants showed good insight into what they learned. By fitting formal learning models, we investigated whether participants learned global functional relationships or made localized predictions from similar experienced exemplars. Both a local (the associative learning model) and a global learning model (the Bayesian linear filter) fitted the data of the first 2 experiments. However, the results of Experiment 3, which was specifically designed to discriminate between local and global learning models, provided more support for global learning models. Finally, we present a novel model to account for the cue competition effects found in previous research and displayed by some of our participants. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Cues,Environment,Feedback,Learning,Models,Probability Learning},
  file = {/home/maarten/Zotero/storage/3YXACTAG/Learning in a Changing Environment.pdf;/home/maarten/Zotero/storage/BX3GMJNA/Speekenbrink and Shanks - 2010 - Learning in a changing environment.pdf;/home/maarten/Zotero/storage/F4QRQPKM/2010-08363-004.html;/home/maarten/Zotero/storage/NQF8TWFW/HTML.html}
}

@article{speekenbrink_uncertainty_2015,
  title = {Uncertainty and {{Exploration}} in a {{Restless Bandit Problem}}},
  author = {Speekenbrink, Maarten and Konstantinidis, Emmanouil},
  year = {2015},
  journal = {Topics in Cognitive Science},
  volume = {7},
  number = {2},
  pages = {351--367},
  issn = {1756-8765},
  doi = {10.1111/tops.12145},
  abstract = {Decision making in noisy and changing environments requires a fine balance between exploiting knowledge about good courses of action and exploring the environment in order to improve upon this knowledge. We present an experiment on a restless bandit task in which participants made repeated choices between options for which the average rewards changed over time. Comparing a number of computational models of participants' behavior in this task, we find evidence that a substantial number of them balanced exploration and exploitation by considering the probability that an option offers the maximum reward out of all the available options.},
  langid = {english},
  keywords = {Dynamic decision making,Exploration-exploitation trade-off,Restless multi-armed bandit task,Uncertainty,Volatility},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/tops.12145},
  file = {/home/maarten/Zotero/storage/S3QXR8MH/Speekenbrink and Konstantinidis - 2015 - Uncertainty and Exploration in a Restless Bandit P.pdf;/home/maarten/Zotero/storage/8GTEUKUC/tops.html}
}

@article{spreng_exploration_2021,
  title = {From Exploration to Exploitation: A Shifting Mental Mode in Late Life Development},
  shorttitle = {From Exploration to Exploitation},
  author = {Spreng, R. Nathan and Turner, Gary R.},
  year = {2021},
  month = dec,
  journal = {Trends in Cognitive Sciences},
  volume = {25},
  number = {12},
  pages = {1058--1071},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2021.09.001},
  abstract = {Changes in cognition, affect, and brain function combine to promote a shift in the nature of mentation in older adulthood, favoring exploitation of prior knowledge over exploratory search as the starting point for thought and action. Age-related exploitation biases result from the accumulation of prior knowledge, reduced cognitive control, and a shift toward affective goals. These are accompanied by changes in cortical networks, as well as attention and reward circuits. By incorporating these factors into a unified account, the exploration-to-exploitation shift offers an integrative model of cognitive, affective, and brain aging. Here, we review evidence for this model, identify determinants and consequences, and survey the challenges and opportunities posed by an exploitation-biased mental mode in later life.},
  langid = {english},
  keywords = {aging,decision making,exploit,explore,older adults},
  file = {/home/maarten/Zotero/storage/7W72CHJ3/Spreng and Turner - 2021 - From exploration to exploitation a shifting menta.pdf;/home/maarten/Zotero/storage/YNMPXB56/S136466132100228X.html}
}

@article{steyvers_bayesian_2009,
  title = {A {{Bayesian}} Analysis of Human Decision-Making on Bandit Problems},
  author = {Steyvers, Mark and Lee, Michael D. and Wagenmakers, Eric-Jan},
  year = {2009},
  month = jun,
  journal = {Journal of Mathematical Psychology},
  series = {Special {{Issue}}: {{Dynamic Decision Making}}},
  volume = {53},
  number = {3},
  pages = {168--179},
  issn = {0022-2496},
  doi = {10.1016/j.jmp.2008.11.002},
  abstract = {The bandit problem is a dynamic decision-making task that is simply described, well-suited to controlled laboratory study, and representative of a broad class of real-world problems. In bandit problems, people must choose between a set of alternatives, each with different unknown reward rates, to maximize the total reward they receive over a fixed number of trials. A key feature of the task is that it challenges people to balance the exploration of unfamiliar choices with the exploitation of familiar ones. We use a Bayesian model of optimal decision-making on the task, in which how people balance exploration with exploitation depends on their assumptions about the distribution of reward rates. We also use Bayesian model selection measures that assess how well people adhere to an optimal decision process, compared to simpler heuristic decision strategies. Using these models, we make inferences about the decision-making of 451 participants who completed a set of bandit problems, and relate various measures of their performance to other psychological variables, including psychometric assessments of cognitive abilities and personality traits. We find clear evidence of individual differences in the way the participants made decisions on the bandit problems, and some interesting correlations with measures of general intelligence.},
  langid = {english},
  keywords = {Bandit problem,Bayesian modeling,Decision-making,Exploration versus exploitation,Individual differences},
  file = {/home/maarten/Zotero/storage/U2UD8QWW/S0022249608001090.html}
}

@article{stojic_its_2020,
  title = {It's New, but Is It Good? {{How}} Generalization and Uncertainty Guide the Exploration of Novel Options},
  shorttitle = {It's New, but Is It Good?},
  author = {Stoji{\'c}, Hrvoje and Schulz, Eric and P. Analytis, Pantelis and Speekenbrink, Maarten},
  year = {2020},
  journal = {Journal of Experimental Psychology: General},
  volume = {149},
  number = {10},
  pages = {1878--1907},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-2222},
  doi = {10.1037/xge0000749},
  abstract = {How do people decide whether to try out novel options as opposed to tried-and-tested ones? We argue that they infer a novel option's reward from contextual information learned from functional relations and take uncertainty into account when making a decision. We propose a Bayesian optimization model to describe their learning and decision making. This model relies on similarity-based learning of functional relationships between features and rewards, and a choice rule that balances exploration and exploitation by combining predicted rewards and the uncertainty of these predictions. Our model makes 2 main predictions. First, decision makers who learn functional relationships will generalize based on the learned reward function, choosing novel options only if their predicted reward is high. Second, they will take uncertainty about the function into account, and prefer novel options that can reduce this uncertainty. We test these predictions in 3 preregistered experiments in which we examine participants' preferences for novel options using a feature-based multiarmed bandit task in which rewards are a noisy function of observable features. Our results reveal strong evidence for functional exploration and moderate evidence for uncertainty-guided exploration. However, whether or not participants chose a novel option also depended on their attention, as well as reflecting on the value of the options. These results advance our understanding of people's reactions in the face of novelty. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  keywords = {Cognition,Decision Making,Generalization (Learning),Learning,Prediction,Rewards,Sensation Seeking,Uncertainty},
  file = {/home/maarten/Zotero/storage/L42M69BT/StojiÄ‡ et al. - 2020 - Itâ€™s new, but is it good How generalization and u.pdf;/home/maarten/Zotero/storage/2NWCZICI/2020-19406-001.html}
}

@article{stojic_uncertainty_2020,
  title = {Uncertainty in Learning, Choice, and Visual Fixation},
  author = {Stoji{\'c}, Hrvoje and Orquin, Jacob L. and Dayan, Peter and Dolan, Raymond J. and Speekenbrink, Maarten},
  year = {2020},
  month = feb,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {117},
  number = {6},
  pages = {3291--3300},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1911348117},
  abstract = {Uncertainty plays a critical role in reinforcement learning and decision making. However, exactly how it influences behavior remains unclear. Multiarmed-bandit tasks offer an ideal test bed, since computational tools such as approximate Kalman filters can closely characterize the interplay between trial-by-trial values, uncertainty, learning, and choice. To gain additional insight into learning and choice processes, we obtained data from subjects' overt allocation of gaze. The estimated value and estimation uncertainty of options influenced what subjects looked at before choosing; these same quantities also influenced choice, as additionally did fixation itself. A momentary measure of uncertainty in the form of absolute prediction errors determined how long participants looked at the obtained outcomes. These findings affirm the importance of uncertainty in multiple facets of behavior and help delineate its effects on decision making.},
  chapter = {Biological Sciences},
  copyright = {\textcopyright{} 2020 . https://www.pnas.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
  langid = {english},
  pmid = {31980535},
  keywords = {decision making,explorationâ€“exploitation,reinforcement learning,uncertainty,visual fixation},
  file = {/home/maarten/Zotero/storage/YJI3H8MJ/StojiÄ‡ et al. - 2020 - Uncertainty in learning, choice, and visual fixati.pdf;/home/maarten/Zotero/storage/LJCGEWFP/3291.html}
}

@book{sutton_reinforcement_2018,
  title = {Reinforcement {{Learning}}: {{An Introduction}}},
  shorttitle = {Reinforcement {{Learning}}, Second Edition},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  year = {2018},
  month = nov,
  edition = {second},
  publisher = {{MIT Press}},
  abstract = {The significantly expanded and updated new edition of a widely used text on reinforcement learning, one of the most active research areas in artificial intelligence.Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms. This second edition has been significantly expanded and updated, presenting new topics and updating coverage of other topics.Like the first edition, this second edition focuses on core online learning algorithms, with the more mathematical material set off in shaded boxes. Part I covers as much of reinforcement learning as possible without going beyond the tabular case for which exact solutions can be found. Many algorithms presented in this part are new to the second edition, including UCB, Expected Sarsa, and Double Learning. Part II extends these ideas to function approximation, with new sections on such topics as artificial neural networks and the Fourier basis, and offers expanded treatment of off-policy learning and policy-gradient methods. Part III has new chapters on reinforcement learning's relationships to psychology and neuroscience, as well as an updated case-studies chapter including AlphaGo and AlphaGo Zero, Atari game playing, and IBM Watson's wagering strategy. The final chapter discusses the future societal impacts of reinforcement learning.},
  googlebooks = {uWV0DwAAQBAJ},
  isbn = {978-0-262-35270-3},
  langid = {english},
  keywords = {Computers / Artificial Intelligence / General}
}

@article{van_lieshout_why_2020,
  title = {Why so Curious? {{Quantifying}} Mechanisms of Information Seeking},
  shorttitle = {Why so Curious?},
  author = {{van Lieshout}, Lieke LF and {de Lange}, Floris P and Cools, Roshan},
  year = {2020},
  month = oct,
  journal = {Current Opinion in Behavioral Sciences},
  series = {Curiosity ({{Explore}} vs {{Exploit}})},
  volume = {35},
  pages = {112--117},
  issn = {2352-1546},
  doi = {10.1016/j.cobeha.2020.08.005},
  abstract = {Humans devote a substantial part of their time to seeking and consuming information. Often, this information is directly relevant. However, we also seek out information without obvious direct purpose. Curiosity about this type of information is called `non-instrumental curiosity'. In this review we ask why we are so curious about information that serves no direct purpose and address the psychological and neural mechanisms by which such apparently purpose-less curiosity is elicited. Non-instrumental curiosity is argued to fulfill (at least) two goals: to progressively reduce uncertainty about the world around us, and to accrue information that makes us feel good. We conclude by highlighting the promise of future psychopharmacological and neurochemical imaging studies of curiosity for elucidating the basis of both state and trait-related variation in curiosity. This is pertinent given the key implication of neurotransmitters like noradrenaline and dopamine in uncertainty reduction, reward motivation and cognitive effort.},
  langid = {english},
  file = {/home/maarten/Zotero/storage/WLES9D6M/van Lieshout et al. - 2020 - Why so curious Quantifying mechanisms of informat.pdf;/home/maarten/Zotero/storage/IY4Q3K6C/S2352154620301248.html}
}

@article{walker_role_2019,
  title = {The Role of Uncertainty in Attentional and Choice Exploration},
  author = {Walker, Adrian R. and Luque, David and Le Pelley, Mike E. and Beesley, Tom},
  year = {2019},
  month = dec,
  journal = {Psychonomic Bulletin \& Review},
  volume = {26},
  number = {6},
  pages = {1911--1916},
  issn = {1531-5320},
  doi = {10.3758/s13423-019-01653-2},
  abstract = {The exploitation-exploration (EE) trade-off describes how, when making a decision, an organism must often choose between a safe alternative with a known pay-off, and one or more riskier alternatives with uncertain pay-offs. Recently, the concept of the EE trade-off has been extended to the examination of how organisms distribute limited attentional resources between several stimuli. This work suggests that when the rules governing the environment are certain, participants learn to ``exploit'' by attending preferentially to cues that provide the most information about upcoming events. However, when the rules are uncertain, people ``explore'' by increasing their attention to all cues that may provide information to help in predicting upcoming events. In the current study, we examine how uncertainty affects the EE trade-off in attention using a contextual two-armed bandit task, where participants explore with both their attention and their choice behavior. We find evidence for an influence of uncertainty on the EE trade-off in both choice and attention. These findings provide support for the idea of an EE trade-off in attention, and that uncertainty is a primary motivator for exploration in both choice and attentional allocation.},
  langid = {english},
  file = {/home/maarten/Zotero/storage/YG6QNHE5/Walker et al. - 2019 - The role of uncertainty in attentional and choice .pdf}
}

@article{wills_predictive_2007,
  title = {Predictive {{Learning}}, {{Prediction Errors}}, and {{Attention}}: {{Evidence}} from {{Event-related Potentials}} and {{Eye Tracking}}},
  shorttitle = {Predictive {{Learning}}, {{Prediction Errors}}, and {{Attention}}},
  author = {Wills, A. J. and Lavric, A. and Croft, G. S. and Hodgson, T. L.},
  year = {2007},
  month = may,
  journal = {Journal of Cognitive Neuroscience},
  volume = {19},
  number = {5},
  pages = {843--854},
  issn = {0898-929X},
  doi = {10.1162/jocn.2007.19.5.843},
  abstract = {Prediction error (``surprise'') affects the rate of learning: We learn more rapidly about cues for which we initially make incorrect predictions than cues for which our initial predictions are correct. The current studies employ electrophysiological measures to reveal early attentional differentiation of events that differ in their previous involvement in errors of predictive judgment. Error-related events attract more attention, as evidenced by features of event-related scalp potentials previously implicated in selective visual attention (selection negativity, augmented anterior N1). The earliest differences detected occurred around 120 msec after stimulus onset, and distributed source localization (LORETA) indicated that the inferior temporal regions were one source of the earliest differences. In addition, stimuli associated with the production of prediction errors show higher dwell times in an eye-tracking procedure. Our data support the view that early attentional processes play a role in human associative learning.},
  file = {/home/maarten/Zotero/storage/DI89RQUU/Wills et al. - 2007 - Predictive Learning, Prediction Errors, and Attent.pdf;/home/maarten/Zotero/storage/3H2DMDEM/Predictive-Learning-Prediction-Errors-and.html}
}

@article{wilson_balancing_2021,
  title = {Balancing Exploration and Exploitation with Information and Randomization},
  author = {Wilson, Robert C and Bonawitz, Elizabeth and Costa, Vincent D and Ebitz, R Becket},
  year = {2021},
  month = apr,
  journal = {Current Opinion in Behavioral Sciences},
  series = {Computational Cognitive Neuroscience},
  volume = {38},
  pages = {49--56},
  issn = {2352-1546},
  doi = {10.1016/j.cobeha.2020.10.001},
  abstract = {Explore-exploit decisions require us to trade off the benefits of exploring unknown options to learn more about them, with exploiting known options, for immediate reward. Such decisions are ubiquitous in nature, but from a computational perspective, they are notoriously hard. There is therefore much interest in how humans and animals make these decisions and recently there has been an explosion of research in this area. Here we provide a biased and incomplete snapshot of this field focusing on the major finding that many organisms use two distinct strategies to solve the explore-exploit dilemma: a bias for information (`directed exploration') and the randomization of choice (`random exploration'). We review evidence for the existence of these strategies, their computational properties, their neural implementations, as well as how directed and random exploration vary over the lifespan. We conclude by highlighting open questions in this field that are ripe to both explore and exploit.},
  langid = {english},
  file = {/home/maarten/Zotero/storage/HCKZ5MGU/Wilson et al. - 2021 - Balancing exploration and exploitation with inform.pdf;/home/maarten/Zotero/storage/FHXPN9XQ/S2352154620301467.html}
}

@article{wilson_humans_2014,
  title = {Humans Use Directed and Random Exploration to Solve the Explore\textendash Exploit Dilemma},
  author = {Wilson, Robert C. and Geana, Andra and White, John M. and Ludvig, Elliot A. and Cohen, Jonathan D.},
  year = {2014},
  journal = {Journal of Experimental Psychology: General},
  volume = {143},
  number = {6},
  pages = {2074--2081},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-2222},
  doi = {10.1037/a0038199},
  abstract = {All adaptive organisms face the fundamental tradeoff between pursuing a known reward (exploitation) and sampling lesser-known options in search of something better (exploration). Theory suggests at least two strategies for solving this dilemma: a directed strategy in which choices are explicitly biased toward information seeking, and a random strategy in which decision noise leads to exploration by chance. In this work we investigated the extent to which humans use these two strategies. In our ``Horizon task,'' participants made explore\textendash exploit decisions in two contexts that differed in the number of choices that they would make in the future (the time horizon). Participants were allowed to make either a single choice in each game (horizon 1), or 6 sequential choices (horizon 6), giving them more opportunity to explore. By modeling the behavior in these two conditions, we were able to measure exploration-related changes in decision making and quantify the contributions of the two strategies to behavior. We found that participants were more information seeking and had higher decision noise with the longer horizon, suggesting that humans use both strategies to solve the exploration\textendash exploitation dilemma. We thus conclude that both information seeking and choice variability can be controlled and put to use in the service of exploration. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Decision Making,Information Seeking,Learning,Reinforcement,Rewards},
  file = {/home/maarten/Zotero/storage/L6G83RW3/Humans Use Directed and Random Exploration to Solve the Exploreâ€“Exploit Dilemma(1).pdf;/home/maarten/Zotero/storage/FG39KYXW/2014-44776-001.html}
}

@article{wise_computational_2019,
  title = {A Computational Account of Threat-Related Attentional Bias},
  author = {Wise, Toby and Michely, Jochen and Dayan, Peter and Dolan, Raymond J.},
  year = {2019},
  month = oct,
  journal = {PLOS Computational Biology},
  volume = {15},
  number = {10},
  pages = {e1007341},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1007341},
  abstract = {Visual selective attention acts as a filter on perceptual information, facilitating learning and inference about important events in an agent's environment. A role for visual attention in reward-based decisions has previously been demonstrated, but it remains unclear how visual attention is recruited during aversive learning, particularly when learning about multiple stimuli concurrently. This question is of particular importance in psychopathology, where enhanced attention to threat is a putative feature of pathological anxiety. Using an aversive reversal learning task that required subjects to learn, and exploit, predictions about multiple stimuli, we show that the allocation of visual attention is influenced significantly by aversive value but not by uncertainty. Moreover, this relationship is bidirectional in that attention biases value updates for attended stimuli, resulting in heightened value estimates. Our findings have implications for understanding biased attention in psychopathology and support a role for learning in the expression of threat-related attentional biases in anxiety.},
  langid = {english},
  keywords = {Animal behavior,Anxiety disorders,Attention,Decision making,Learning,Probability distribution,Sensory perception,Vision},
  file = {/home/maarten/Zotero/storage/HP6G5W9Q/Wise et al. - 2019 - A computational account of threat-related attentio.pdf;/home/maarten/Zotero/storage/EBZ2DL8C/article.html}
}

@article{wu_generalization_2018,
  title = {Generalization Guides Human Exploration in Vast Decision Spaces},
  author = {Wu, Charley M. and Schulz, Eric and Speekenbrink, Maarten and Nelson, Jonathan D. and Meder, Bj{\"o}rn},
  year = {2018},
  month = dec,
  journal = {Nature Human Behaviour},
  volume = {2},
  number = {12},
  pages = {915--924},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0467-4},
  abstract = {From foraging for food to learning complex games, many aspects of human behaviour can be framed as a search problem with a vast space of possible actions. Under finite search horizons, optimal solutions are generally unobtainable. Yet, how do humans navigate vast problem spaces, which require intelligent exploration of unobserved actions? Using various bandit tasks with up to 121 arms, we study how humans search for rewards under limited search horizons, in which the spatial correlation of rewards (in both generated and natural environments) provides traction for generalization. Across various different probabilistic and heuristic models, we find evidence that Gaussian process function learning\textemdash combined with an optimistic upper confidence bound sampling strategy\textemdash provides a robust account of how people use generalization to guide search. Our modelling results and parameter estimates are recoverable and can be used to simulate human-like performance, providing insights about human behaviour in complex environments.},
  copyright = {2018 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Computational science,Human behaviour},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Computational science;Human behaviour Subject\_term\_id: computational-science;human-behaviour},
  file = {/home/maarten/Zotero/storage/2JIYFDYE/Wu et al. - 2018 - Generalization guides human exploration in vast de.pdf;/home/maarten/Zotero/storage/DCDCB3SH/s41562-018-0467-4.html}
}

@misc{wu_time_2021,
  title = {Time to Explore: {{Adaptation}} of Exploration under Time Pressure},
  shorttitle = {Time to Explore},
  author = {Wu, Charley M. and Schulz, Eric and Pleskac, Tim and Speekenbrink, Maarten},
  year = {2021},
  month = jul,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/dsw7q},
  abstract = {How does time pressure influence exploration and decision-making? We investigate this question using a within-subject design to manipulate decision time (limited vs. unlimited) and use a range of four-armed bandit tasks, designed to independently manipulate uncertainty and expected reward. With limited time, people have less opportunity to perform costly computations, thus shifting the cost-benefit balance of different exploration strategies. Through behavioral, reinforcement learning (RL), reaction time (RT), and evidence accumulation analyses, we show that time pressure changes how people explore and respond to uncertainty. Specifically, participants reduced their uncertainty-directed exploration under time pressure, were less value-directed, and repeated choices more often. Since our analyses relate uncertainty to slower responses and dampened evidence accumulation (i.e., drift rates), this demonstrates a resource-rational shift towards simpler, lower-cost strategies under time pressure.  These results shed light on how people adapt their exploration and decision-making strategies to externally imposed cognitive constraints.},
  langid = {american},
  keywords = {Cognitive Psychology,Computational Modeling,evidence accumulation,exploration-exploitation,Judgment and Decision Making,Learning,Life Sciences,multi-armed bandits,Quantitative Methods,reaction times,reinforcement learning,resource-rationality,Social and Behavioral Sciences,Time pressure,uncertainty},
  file = {/home/maarten/Zotero/storage/VF76C3MK/Wu et al. - 2021 - Time to explore Adaptation of exploration under t.pdf}
}

@article{wulff_how_2015,
  title = {How Short- and Long-Run Aspirations Impact Search and Choice in Decisions from Experience},
  author = {Wulff, Dirk U. and Hills, Thomas T. and Hertwig, Ralph},
  year = {2015},
  month = nov,
  journal = {Cognition},
  volume = {144},
  pages = {29--37},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2015.07.006},
  abstract = {To what extent do people adapt their information search policies and subsequent decisions to the long- and short-run consequences of choice environments? To address this question, we investigated exploration and exploitation policies in choice environments that involved single or multiple plays. We further compared behavior in these environments with behavior in the standard sampling paradigm. Frequently used in research on decision from experience, this paradigm does not explicitly implement the choice in terms of the short or long run. Results showed that people searched more in the multi-play environment than in the single-play environment. Moreover, the substantial search effort in the multi-play environment was conducive to choices consistent with expected value maximization, whereas the lesser search effort in the single-play environment was compatible with the goal of maximizing the chance of winning something. Furthermore, choice and search behaviors in the sampling paradigm predominantly echoed those observed in the single-play environment. This suggests that, when not instructed otherwise, participants in the sampling paradigm appear to favor search and choice strategies that embody short-run aspirations. Finally, the present findings challenge the revealed preference approach in decisions from experience, while also suggesting that information search may be an important and potentially even better signal of preference or aspirations than choice.},
  langid = {english},
  keywords = {Decisions from experience,Information sampling,One-shot and multi-play gambles,Risky choice},
  file = {/home/maarten/Zotero/storage/WARQ269Q/Wulff et al. - 2015 - How short- and long-run aspirations impact search .pdf;/home/maarten/Zotero/storage/6UASUN4L/S0010027715300329.html}
}

@article{yu_uncertainty_2005,
  title = {Uncertainty, {{Neuromodulation}}, and {{Attention}}},
  author = {Yu, Angela J. and Dayan, Peter},
  year = {2005},
  month = may,
  journal = {Neuron},
  volume = {46},
  number = {4},
  pages = {681--692},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2005.04.026},
  abstract = {Uncertainty in various forms plagues our interactions with the environment. In a Bayesian statistical framework, optimal inference and prediction, based on unreliable observations in changing contexts, require the representation and manipulation of different forms of uncertainty. We propose that the neuromodulators acetylcholine and norepinephrine play a major role in the brain's implementation of these uncertainty computations. Acetylcholine signals expected uncertainty, coming from known unreliability of predictive cues within a context. Norepinephrine signals unexpected uncertainty, as when unsignaled context switches produce strongly unexpected observations. These uncertainty signals interact to enable optimal inference and learning in noisy and changeable environments. This formulation is consistent with a wealth of physiological, pharmacological, and behavioral data implicating acetylcholine and norepinephrine in specific aspects of a range of cognitive processes. Moreover, the model suggests a class of attentional cueing tasks that involve both neuromodulators and shows how their interactions may be part-antagonistic, part-synergistic.},
  langid = {english},
  file = {/home/maarten/Zotero/storage/HLITZTKT/Yu and Dayan - 2005 - Uncertainty, Neuromodulation, and Attention.pdf;/home/maarten/Zotero/storage/K46HZ3JU/S0896627305003624.html}
}


